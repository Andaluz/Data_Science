{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will be using deep learning techniques on the MNIST database. The MNIST (Modified National Institute of Standards and Technology) database is a large collection of handwritten digits that is commonly used for training various image processing systems. Let's begin with importing our needed libraries and exploring the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPool2D, Dropout, Activation, Dense, Flatten\n",
    "from keras.activations import relu, softmax\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [0 4 1 9 2 1 3 1 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's import the data and visualize a sample #\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('Target:', y_train[1:10])\n",
    "fig = plt.figure(33)\n",
    "for i in range(1,10):\n",
    "    fig.add_subplot(330+i)\n",
    "    plt.imshow(X_train[i], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the shape of the data #\n",
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.uint8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the data type #\n",
    "type(X_train[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(28, 28)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check if all data shapes are the same 28x28 pixels #\n",
    "set([i.shape for i in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize the data (pixel values of images are between 0 and 255) #\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's one-hot encode our Y's #\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are a vector of probabilities, based on what category it will fall underneath #\n",
    "# Classification with deep learning will always make one neuron for each category #\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the image format #\n",
    "keras.backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the shape of our X_train dataset #\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To explicitly say there's 1 channel, we need to add a 1 to the shape #\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the shape of our X_train dataset again #\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the shape of our y_train dataset #\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the Multi-Layer Perceptron (MLP) model. This involves an input layer, a layer to flatten the data, and an output layer. To build this, we will use the sequential model. That will allow us to add layers in a sequential list, compile the model with an optimizer, loss function, and scorint metric; and ultimately fit the model to the data.\n",
    "\n",
    "It should be noted that the first layer has an input_shape parameter. Because we are setting 24 output neurons, and 28 inputs to those neurons, the output of that layer is 28x28x24.\n",
    "\n",
    "The next layer, flatten, simply turns the 3d input into a vector of length 18816 (28x28x24 = 18816).\n",
    "\n",
    "In the last layer, we have a dense layer with an output shape of 10 neurons and a softmax activation. This results in 10 output neurons that encode probabilities of each class (digits 0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0509 16:03:25.607047  9988 deprecation_wrapper.py:119] From C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0509 16:03:25.627058  9988 deprecation_wrapper.py:119] From C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0509 16:03:25.636837  9988 deprecation_wrapper.py:119] From C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate our model #\n",
    "mlp = Sequential([\n",
    "    Dense(24, input_shape=(28,28,1), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 28, 28, 24)        48        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18816)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                188170    \n",
      "=================================================================\n",
      "Total params: 188,218\n",
      "Trainable params: 188,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's check out the model summary #\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0509 16:03:25.766531  9988 deprecation_wrapper.py:119] From C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0509 16:03:25.769552  9988 deprecation_wrapper.py:119] From C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's compile our model (we are doing a multi-class classification to minimize loss) #\n",
    "mlp.compile(optimizer=keras.optimizers.SGD(), \n",
    "           loss=keras.losses.categorical_crossentropy,\n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0509 16:03:25.886556  9988 deprecation.py:323] From C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0509 16:03:25.976852  9988 deprecation_wrapper.py:119] From C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.5129 - acc: 0.8684 - val_loss: 0.3130 - val_acc: 0.9113\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.3161 - acc: 0.9099 - val_loss: 0.2903 - val_acc: 0.9174\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.2971 - acc: 0.9154 - val_loss: 0.2862 - val_acc: 0.9194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a5a75d4a8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's fit our model #\n",
    "mlp.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 82us/step\n",
      "\n",
      "Loss: 0.2861699765682221\n",
      "Accuracy: 0.9194\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate our loss and accuracy #\n",
    "loss, accuracy = mlp.evaluate(X_test, y_test)\n",
    "print('\\nLoss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate our model again (changing to 48 output neurons in the first layer) #\n",
    "mlp_48 = Sequential([\n",
    "    Dense(48, input_shape=(28,28,1), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compile our model #\n",
    "mlp_48.compile(optimizer=keras.optimizers.SGD(), \n",
    "           loss=keras.losses.categorical_crossentropy, \n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/3\n",
      "42000/42000 [==============================] - 26s 611us/step - loss: 0.5799 - acc: 0.8598 - val_loss: 0.3541 - val_acc: 0.8992\n",
      "Epoch 2/3\n",
      "42000/42000 [==============================] - 28s 677us/step - loss: 0.3333 - acc: 0.9044 - val_loss: 0.3166 - val_acc: 0.9089\n",
      "Epoch 3/3\n",
      "42000/42000 [==============================] - 25s 590us/step - loss: 0.3070 - acc: 0.9114 - val_loss: 0.3029 - val_acc: 0.9141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a5aa819e8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's fit our model with a validation split #\n",
    "mlp_48.fit(X_train, y_train, validation_split=0.3, shuffle=True, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 160us/step\n",
      "\n",
      "Loss: 0.2916765928775072\n",
      "Accuracy: 0.9171\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate our loss and accuracy #\n",
    "loss, accuracy = mlp_48.evaluate(X_test, y_test)\n",
    "print('\\nLoss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by creating a Convolutional Neural Network (CNN). This should not only train faster, but should obtain better scores. Let's play around with the CNN parameters to see how tuning can improve our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0509 16:05:30.496526  9988 deprecation_wrapper.py:119] From C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate our model #\n",
    "cnn = Sequential([\n",
    "    Conv2D(2, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 2)         52        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 2)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                18496     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 19,198\n",
      "Trainable params: 19,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's check out the model summary #\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compile our model #\n",
    "cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/3\n",
      "42000/42000 [==============================] - 16s 389us/step - loss: 0.2706 - acc: 0.9175 - val_loss: 0.1822 - val_acc: 0.9464\n",
      "Epoch 2/3\n",
      "42000/42000 [==============================] - 17s 415us/step - loss: 0.1260 - acc: 0.9622 - val_loss: 0.1235 - val_acc: 0.9642\n",
      "Epoch 3/3\n",
      "42000/42000 [==============================] - 14s 335us/step - loss: 0.0990 - acc: 0.9704 - val_loss: 0.1191 - val_acc: 0.9646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a5a92c588>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's fit our model with a validation split #\n",
    "cnn.fit(X_train, y_train, validation_split=.3, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 121us/step\n",
      "\n",
      "Loss: 0.1010375030733645\n",
      "Accuracy: 0.9687\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate our loss and accuracy #\n",
    "loss, accuracy = cnn.evaluate(X_test, y_test)\n",
    "print('\\nLoss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0509 16:06:20.206864  9988 deprecation.py:506] From C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate our model again (changing to 8 output neurons in the first layer) #\n",
    "cnn_8 = Sequential([\n",
    "    Conv2D(8, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)),\n",
    "     Conv2D(16, (3, 3), activation='relu'),\n",
    "     Dropout(.25),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "     Dropout(.25),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 8)         208       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 22, 22, 16)        1168      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1936)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                123968    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 125,994\n",
      "Trainable params: 125,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's check out the model summary #\n",
    "cnn_8.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compile our model #\n",
    "cnn_8.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/3\n",
      "42000/42000 [==============================] - 37s 870us/step - loss: 0.2265 - acc: 0.9302 - val_loss: 0.0824 - val_acc: 0.9760\n",
      "Epoch 2/3\n",
      "42000/42000 [==============================] - 42s 990us/step - loss: 0.0884 - acc: 0.9736 - val_loss: 0.0710 - val_acc: 0.9776\n",
      "Epoch 3/3\n",
      "42000/42000 [==============================] - 41s 973us/step - loss: 0.0680 - acc: 0.9795 - val_loss: 0.0557 - val_acc: 0.9834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a5b6092b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's fit our model with a validation split #\n",
    "cnn_8.fit(X_train, y_train, validation_split=.3, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 162us/step\n",
      "\n",
      "Loss: 0.04308855373323895\n",
      "Accuracy: 0.9865\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate our loss and accuracy #\n",
    "loss, accuracy = cnn_8.evaluate(X_test, y_test)\n",
    "print('\\nLoss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.9275881e-08, 3.5568501e-08, 9.0037975e-06, 9.0894773e-06,\n",
       "       4.4193480e-09, 2.6133952e-07, 1.7786893e-11, 9.9997759e-01,\n",
       "       1.7684272e-06, 2.1261133e-06], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a vector of probabilities #\n",
    "y_pred_probas = cnn_8.predict(X_test)\n",
    "\n",
    "y_pred_probas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 8th value in the pred_probas above is close to 1 just like the actual y_test value #\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves model in same directory #\n",
    "cnn.save('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/3\n",
      "42000/42000 [==============================] - 14s 328us/step - loss: 0.0830 - acc: 0.9746 - val_loss: 0.0935 - val_acc: 0.9722\n",
      "Epoch 2/3\n",
      "42000/42000 [==============================] - 13s 316us/step - loss: 0.0726 - acc: 0.9789 - val_loss: 0.0925 - val_acc: 0.9731\n",
      "Epoch 3/3\n",
      "42000/42000 [==============================] - 16s 375us/step - loss: 0.0650 - acc: 0.9806 - val_loss: 0.0884 - val_acc: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a5d36c828>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we re-run the fit, it will continue training where it left off previously #\n",
    "cnn2 = keras.models.load_model('./cnn.h5')\n",
    "\n",
    "cnn2.fit(X_train, y_train, validation_split=.3, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 169us/step\n",
      "\n",
      "Loss: 0.07341869105740916\n",
      "Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "# Let's check our loss and accuracy again #\n",
    "loss, accuracy = cnn2.evaluate(X_test, y_test)\n",
    "print('\\nLoss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: what we did above can be very useful if a specific accuracy figure is needed as we can just set a while loop to keep on training until we reach our goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
