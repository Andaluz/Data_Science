{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Here, we will create a web scraper to scrape OpenTable's DC listings. We're interested in knowing the restaurant's name, location, price, and how many people booked it today. OpenTable provides all of this information on their website page: http://www.opentable.com/washington-dc-restaurant-listings. Let's inspect the elements of this page to assure we can find each of the bits of information in which we're interested and begin with importing our needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import twitter, re, datetime, pandas as pd\n",
    "from textacy import preprocessing\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Let's set the url we want to visit #\n",
    "url = 'http://www.opentable.com/washington-dc-restaurant-listings'\n",
    "\n",
    "# Let's visit that url and grab the html #\n",
    "html = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'           <!DOCTYPE html><html lang=\"en\"><head><meta charset=\"utf-8\"/><meta http-equiv=\"X-UA-Compatible\" content=\"IE=9; IE=8; IE=7; IE=EDGE\"/> <title>Restaurant Reservation Availability</title>    <meta  name=\"robots\" content=\"noindex,nofollow\" > </meta>     <link rel=\"shortcut icon\" href=\"//components.otstatic.com/components/favicon/1.0.6/favicon/favicon.ico\" type=\"image/x-icon\"/><link rel=\"icon\" href=\"//components.otstatic.com/components/favicon/1.0.6/favicon/favicon-16.png\" sizes=\"16x16\"/><l'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check what's in the html (.text returns the request content in Unicode) #\n",
    "html.text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Let's convert into a soup object so we can parse it #\n",
    "soup = BeautifulSoup(html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Note: We will utilize the web browser inspect tool to find the tags associated with elements of the page we want to scrape.\n",
    "\n",
    "As a soup object, we can now begin to retrieve data from the HTML page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"rest-row-name-text\">Et Corner</span>,\n",
       " <span class=\"rest-row-name-text\">Napoleon Ondricka</span>,\n",
       " <span class=\"rest-row-name-text\">922 O'Reilly</span>,\n",
       " <span class=\"rest-row-name-text\">Zackary Lakin</span>,\n",
       " <span class=\"rest-row-name-text\">Murray</span>,\n",
       " <span class=\"rest-row-name-text\">Doloremque Burgs</span>,\n",
       " <span class=\"rest-row-name-text\">Villages</span>,\n",
       " <span class=\"rest-row-name-text\">Cum Mill</span>,\n",
       " <span class=\"rest-row-name-text\">Nesciunt Avenue</span>,\n",
       " <span class=\"rest-row-name-text\">429 Kohler</span>,\n",
       " <span class=\"rest-row-name-text\">Schuster</span>,\n",
       " <span class=\"rest-row-name-text\">Minus Vista</span>,\n",
       " <span class=\"rest-row-name-text\">Magnam Place</span>,\n",
       " <span class=\"rest-row-name-text\">Blanditiis Fords</span>,\n",
       " <span class=\"rest-row-name-text\">Kaileys</span>,\n",
       " <span class=\"rest-row-name-text\">Treutel</span>,\n",
       " <span class=\"rest-row-name-text\">Libero</span>,\n",
       " <span class=\"rest-row-name-text\">Quia Shields</span>,\n",
       " <span class=\"rest-row-name-text\">Iure</span>,\n",
       " <span class=\"rest-row-name-text\">Hazles</span>,\n",
       " <span class=\"rest-row-name-text\">Consequatur</span>,\n",
       " <span class=\"rest-row-name-text\">Fredericks</span>,\n",
       " <span class=\"rest-row-name-text\">856 Nader</span>,\n",
       " <span class=\"rest-row-name-text\">Suscipit</span>,\n",
       " <span class=\"rest-row-name-text\">Kellis</span>,\n",
       " <span class=\"rest-row-name-text\">Kertzmann Keys</span>,\n",
       " <span class=\"rest-row-name-text\">Agloe Bar &amp; Grill</span>,\n",
       " <span class=\"rest-row-name-text\">Lauras</span>,\n",
       " <span class=\"rest-row-name-text\">Exercitationem Schimmel</span>,\n",
       " <span class=\"rest-row-name-text\">Ut</span>,\n",
       " <span class=\"rest-row-name-text\">Annabelle Center</span>,\n",
       " <span class=\"rest-row-name-text\">Brandi Barton</span>,\n",
       " <span class=\"rest-row-name-text\">Illo Court</span>,\n",
       " <span class=\"rest-row-name-text\">1161 Kemmer</span>,\n",
       " <span class=\"rest-row-name-text\">Iure Branch</span>,\n",
       " <span class=\"rest-row-name-text\">Corrupti Waelchi</span>,\n",
       " <span class=\"rest-row-name-text\">Padberg</span>,\n",
       " <span class=\"rest-row-name-text\">Dolorem Creek</span>,\n",
       " <span class=\"rest-row-name-text\">Granvilles</span>,\n",
       " <span class=\"rest-row-name-text\">542 Erdman</span>,\n",
       " <span class=\"rest-row-name-text\">Quisquam D'Amore</span>,\n",
       " <span class=\"rest-row-name-text\">Perferendis Locks</span>,\n",
       " <span class=\"rest-row-name-text\">Soluta Corners</span>,\n",
       " <span class=\"rest-row-name-text\">Non Vista</span>,\n",
       " <span class=\"rest-row-name-text\">Jasmin Koss</span>,\n",
       " <span class=\"rest-row-name-text\">Casper</span>,\n",
       " <span class=\"rest-row-name-text\">618 Larson</span>,\n",
       " <span class=\"rest-row-name-text\">Laborum Groves</span>,\n",
       " <span class=\"rest-row-name-text\">Izabella Will</span>,\n",
       " <span class=\"rest-row-name-text\">Donnell Haag</span>,\n",
       " <span class=\"rest-row-name-text\">Veronica Anderson</span>,\n",
       " <span class=\"rest-row-name-text\">Buster Falls</span>,\n",
       " <span class=\"rest-row-name-text\">Considine</span>,\n",
       " <span class=\"rest-row-name-text\">Mitchell Plains</span>,\n",
       " <span class=\"rest-row-name-text\">Chester Radial</span>,\n",
       " <span class=\"rest-row-name-text\">Emmas</span>,\n",
       " <span class=\"rest-row-name-text\">1448 Gaylord</span>,\n",
       " <span class=\"rest-row-name-text\">Carloss</span>,\n",
       " <span class=\"rest-row-name-text\">Village</span>,\n",
       " <span class=\"rest-row-name-text\">Via</span>,\n",
       " <span class=\"rest-row-name-text\">Hickle</span>,\n",
       " <span class=\"rest-row-name-text\">Est</span>,\n",
       " <span class=\"rest-row-name-text\">Erdman Islands</span>,\n",
       " <span class=\"rest-row-name-text\">Coralie Kessler</span>,\n",
       " <span class=\"rest-row-name-text\">Sunt</span>,\n",
       " <span class=\"rest-row-name-text\">535 Ortiz</span>,\n",
       " <span class=\"rest-row-name-text\">Langosh Lake</span>,\n",
       " <span class=\"rest-row-name-text\">Minima Jaskolski</span>,\n",
       " <span class=\"rest-row-name-text\">Oswaldo Legros</span>,\n",
       " <span class=\"rest-row-name-text\">Recusandae Deckow</span>,\n",
       " <span class=\"rest-row-name-text\">Island</span>,\n",
       " <span class=\"rest-row-name-text\">Vitae</span>,\n",
       " <span class=\"rest-row-name-text\">Mountains</span>,\n",
       " <span class=\"rest-row-name-text\">Ikes</span>,\n",
       " <span class=\"rest-row-name-text\">Scarletts</span>,\n",
       " <span class=\"rest-row-name-text\">Volkman Flat</span>,\n",
       " <span class=\"rest-row-name-text\">Quaerat Light</span>,\n",
       " <span class=\"rest-row-name-text\">1188 Dietrich</span>,\n",
       " <span class=\"rest-row-name-text\">Consectetur Field</span>,\n",
       " <span class=\"rest-row-name-text\">Electas</span>,\n",
       " <span class=\"rest-row-name-text\">427 Little</span>,\n",
       " <span class=\"rest-row-name-text\">524 Hettinger</span>,\n",
       " <span class=\"rest-row-name-text\">Drives</span>,\n",
       " <span class=\"rest-row-name-text\">Dolorem Lakes</span>,\n",
       " <span class=\"rest-row-name-text\">Raphaelles</span>,\n",
       " <span class=\"rest-row-name-text\">Mason Ports</span>,\n",
       " <span class=\"rest-row-name-text\">Karlee Bartoletti</span>,\n",
       " <span class=\"rest-row-name-text\">Quo Pine</span>,\n",
       " <span class=\"rest-row-name-text\">Gateway</span>,\n",
       " <span class=\"rest-row-name-text\">Laborum</span>,\n",
       " <span class=\"rest-row-name-text\">1037 Feeney</span>,\n",
       " <span class=\"rest-row-name-text\">Ullam</span>,\n",
       " <span class=\"rest-row-name-text\">Lang Highway</span>,\n",
       " <span class=\"rest-row-name-text\">Llewellyn Reinger</span>,\n",
       " <span class=\"rest-row-name-text\">Beatae Stroman</span>,\n",
       " <span class=\"rest-row-name-text\">Florian Islands</span>,\n",
       " <span class=\"rest-row-name-text\">Lodge</span>,\n",
       " <span class=\"rest-row-name-text\">Koelpin</span>,\n",
       " <span class=\"rest-row-name-text\">Aut</span>,\n",
       " <span class=\"rest-row-name-text\">Batz Haven</span>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the restaurant names #\n",
    "soup.find_all(name='span', attrs={'class':'rest-row-name-text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et Corner\n",
      "Napoleon Ondricka\n",
      "922 O'Reilly\n",
      "Zackary Lakin\n",
      "Murray\n",
      "Doloremque Burgs\n",
      "Villages\n",
      "Cum Mill\n",
      "Nesciunt Avenue\n",
      "429 Kohler\n",
      "Schuster\n",
      "Minus Vista\n",
      "Magnam Place\n",
      "Blanditiis Fords\n",
      "Kaileys\n",
      "Treutel\n",
      "Libero\n",
      "Quia Shields\n",
      "Iure\n",
      "Hazles\n",
      "Consequatur\n",
      "Fredericks\n",
      "856 Nader\n",
      "Suscipit\n",
      "Kellis\n",
      "Kertzmann Keys\n",
      "Agloe Bar & Grill\n",
      "Lauras\n",
      "Exercitationem Schimmel\n",
      "Ut\n",
      "Annabelle Center\n",
      "Brandi Barton\n",
      "Illo Court\n",
      "1161 Kemmer\n",
      "Iure Branch\n",
      "Corrupti Waelchi\n",
      "Padberg\n",
      "Dolorem Creek\n",
      "Granvilles\n",
      "542 Erdman\n",
      "Quisquam D'Amore\n",
      "Perferendis Locks\n",
      "Soluta Corners\n",
      "Non Vista\n",
      "Jasmin Koss\n",
      "Casper\n",
      "618 Larson\n",
      "Laborum Groves\n",
      "Izabella Will\n",
      "Donnell Haag\n",
      "Veronica Anderson\n",
      "Buster Falls\n",
      "Considine\n",
      "Mitchell Plains\n",
      "Chester Radial\n",
      "Emmas\n",
      "1448 Gaylord\n",
      "Carloss\n",
      "Village\n",
      "Via\n",
      "Hickle\n",
      "Est\n",
      "Erdman Islands\n",
      "Coralie Kessler\n",
      "Sunt\n",
      "535 Ortiz\n",
      "Langosh Lake\n",
      "Minima Jaskolski\n",
      "Oswaldo Legros\n",
      "Recusandae Deckow\n",
      "Island\n",
      "Vitae\n",
      "Mountains\n",
      "Ikes\n",
      "Scarletts\n",
      "Volkman Flat\n",
      "Quaerat Light\n",
      "1188 Dietrich\n",
      "Consectetur Field\n",
      "Electas\n",
      "427 Little\n",
      "524 Hettinger\n",
      "Drives\n",
      "Dolorem Lakes\n",
      "Raphaelles\n",
      "Mason Ports\n",
      "Karlee Bartoletti\n",
      "Quo Pine\n",
      "Gateway\n",
      "Laborum\n",
      "1037 Feeney\n",
      "Ullam\n",
      "Lang Highway\n",
      "Llewellyn Reinger\n",
      "Beatae Stroman\n",
      "Florian Islands\n",
      "Lodge\n",
      "Koelpin\n",
      "Aut\n",
      "Batz Haven\n"
     ]
    }
   ],
   "source": [
    "# Let's print out the restaurant names for each element we find #\n",
    "for entry in soup.find_all(name='span', attrs={'class':'rest-row-name-text'}):\n",
    "    print(entry.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Toneystad</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Port Lilianaville</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">North Kassandra</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Koeppbury</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">New Karltown</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Cielo</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Watsicaburgh</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Pacochaside</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Schaefermouth</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Keeblerside</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Lake Aronhaven</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Vinnie</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Lake Natalie</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">South Scotburgh</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Felipabury</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Abbottville</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Lydaburgh</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">New Anissa</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Jacobiport</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Walterview</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">South Toreyhaven</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Maye</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Eugenialand</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Ratkeside</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Cheyenneville</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Halvorsonburgh</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">East Alaynaside</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Cierraborough</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Traceymouth</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Lake Jakobfort</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Judyville</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">East Jamelmouth</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">East Keyon</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Lake Josefina</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">South Destini</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Port Marisaberg</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Lake Vitaborough</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">New Ottostad</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Schoenhaven</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Port Norris</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Nyah</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Maverickborough</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Lake Moriahbury</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Dino</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Kelli</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Addiemouth</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Ramonfurt</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Walkerborough</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">New Hillary</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Lake Emelie</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Treutelhaven</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Elinorehaven</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">New Justus</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Port Mabelleberg</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Colbyside</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Trinityton</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Shanon</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">North Johnathon</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Auerton</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">New Nora</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Kassulkefurt</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Kareemside</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Lelaside</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Grahammouth</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Vitoport</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Bergstromchester</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">New Fernefurt</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Tad</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">New Taylorland</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Toyville</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Rosebury</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">East Korbin</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">North Heavenberg</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Kutchburgh</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Demetrisside</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Lake Maximohaven</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Jayceechester</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Harveyville</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Tyrelside</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">East Arlo</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Port Rosanna</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Hellerborough</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Reynoldsberg</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Kareembury</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">East Ezequiel</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Schowalterport</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Prestonfort</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Vickiestad</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Medhurststad</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Westton</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Port Anikashire</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Nitzschechester</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Mylenechester</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Flatleyview</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Heidenreichton</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">West Ayden</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Port Madelynn</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Nataliechester</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Marquardtville</span>,\n",
       " <span class=\"rest-row-meta--location rest-row-meta-text sfx1388addContent\">Bertrandfort</span>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the restaurant locations #\n",
    "soup.find_all(name='span', attrs={'class':'rest-row-meta--location'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Toneystad\n",
      "Port Lilianaville\n",
      "North Kassandra\n",
      "Koeppbury\n",
      "New Karltown\n",
      "West Cielo\n",
      "Watsicaburgh\n",
      "Pacochaside\n",
      "Schaefermouth\n",
      "Keeblerside\n",
      "Lake Aronhaven\n",
      "West Vinnie\n",
      "Lake Natalie\n",
      "South Scotburgh\n",
      "Felipabury\n",
      "Abbottville\n",
      "Lydaburgh\n",
      "New Anissa\n",
      "Jacobiport\n",
      "Walterview\n",
      "South Toreyhaven\n",
      "West Maye\n",
      "Eugenialand\n",
      "Ratkeside\n",
      "Cheyenneville\n",
      "Halvorsonburgh\n",
      "East Alaynaside\n",
      "West Cierraborough\n",
      "Traceymouth\n",
      "Lake Jakobfort\n",
      "Judyville\n",
      "East Jamelmouth\n",
      "East Keyon\n",
      "Lake Josefina\n",
      "South Destini\n",
      "Port Marisaberg\n",
      "Lake Vitaborough\n",
      "New Ottostad\n",
      "Schoenhaven\n",
      "Port Norris\n",
      "West Nyah\n",
      "Maverickborough\n",
      "Lake Moriahbury\n",
      "West Dino\n",
      "West Kelli\n",
      "Addiemouth\n",
      "Ramonfurt\n",
      "Walkerborough\n",
      "New Hillary\n",
      "Lake Emelie\n",
      "Treutelhaven\n",
      "Elinorehaven\n",
      "New Justus\n",
      "Port Mabelleberg\n",
      "Colbyside\n",
      "Trinityton\n",
      "West Shanon\n",
      "North Johnathon\n",
      "Auerton\n",
      "New Nora\n",
      "Kassulkefurt\n",
      "West Kareemside\n",
      "Lelaside\n",
      "Grahammouth\n",
      "Vitoport\n",
      "Bergstromchester\n",
      "New Fernefurt\n",
      "West Tad\n",
      "New Taylorland\n",
      "Toyville\n",
      "Rosebury\n",
      "East Korbin\n",
      "North Heavenberg\n",
      "Kutchburgh\n",
      "Demetrisside\n",
      "Lake Maximohaven\n",
      "Jayceechester\n",
      "Harveyville\n",
      "West Tyrelside\n",
      "East Arlo\n",
      "Port Rosanna\n",
      "Hellerborough\n",
      "Reynoldsberg\n",
      "Kareembury\n",
      "East Ezequiel\n",
      "Schowalterport\n",
      "Prestonfort\n",
      "West Vickiestad\n",
      "Medhurststad\n",
      "Westton\n",
      "Port Anikashire\n",
      "Nitzschechester\n",
      "West Mylenechester\n",
      "Flatleyview\n",
      "Heidenreichton\n",
      "West Ayden\n",
      "Port Madelynn\n",
      "Nataliechester\n",
      "Marquardtville\n",
      "Bertrandfort\n"
     ]
    }
   ],
   "source": [
    "# Let's print out the restaurant locations for each element we find #\n",
    "for entry in soup.find_all('span', {'class':'rest-row-meta--location'}):\n",
    "    print(entry.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $      </i> <span class=\"pricing--not-the-price\">  $    $      </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    $  </i> <span class=\"pricing--not-the-price\"> </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>,\n",
       " <div class=\"rest-row-pricing\"> <i class=\"pricing--the-price\">  $    $    $    </i> <span class=\"pricing--not-the-price\">  $        </span></div>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the restaurant prices #\n",
    "soup.find_all('div', {'class':'rest-row-pricing'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $      \n",
      "  $    $    $    $  \n",
      "  $    $      \n",
      "  $    $      \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $      \n",
      "  $    $      \n",
      "  $    $      \n",
      "  $    $      \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $    $    $  \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $      \n",
      "  $    $    $    $  \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $      \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $    $    \n",
      "  $    $    $    \n",
      "  $    $    $    \n",
      "  $    $    $    \n",
      "  $    $    $    \n",
      "  $    $    $    \n",
      "  $    $    $    \n",
      "  $    $    $    $  \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    \n",
      "  $    $      \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    $  \n",
      "  $    $    $    \n",
      "  $    $    $    \n"
     ]
    }
   ],
   "source": [
    "# Let's print out the restaurant prices (dollar signs) for each element we find #\n",
    "for entry in soup.find_all('div', {'class':'rest-row-pricing'}):\n",
    "    print(entry.find('i').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 2\n",
      "Number of $: 4\n",
      "Number of $: 2\n",
      "Number of $: 2\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 2\n",
      "Number of $: 2\n",
      "Number of $: 2\n",
      "Number of $: 2\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 4\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 2\n",
      "Number of $: 4\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 2\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 3\n",
      "Number of $: 3\n",
      "Number of $: 3\n",
      "Number of $: 3\n",
      "Number of $: 3\n",
      "Number of $: 3\n",
      "Number of $: 3\n",
      "Number of $: 4\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 3\n",
      "Number of $: 2\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 4\n",
      "Number of $: 3\n",
      "Number of $: 3\n"
     ]
    }
   ],
   "source": [
    "# Let's try to print the number of dollars signs per restaurant #\n",
    "for entry in soup.find_all('div', {'class':'rest-row-pricing'}):\n",
    "    price = entry.find('i').text\n",
    "    print('Number of $:',price.count('$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the number of times each restaurant was booked #\n",
    "soup.find_all('div', {'class':'booking'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "It seems like we can't find the number of bookings for each resturant. This may be due to the fact that the number of bookings can be considered as dynamic data (as opposed to the restaurant's name, location, and price, which can be considered as static data). Thus, we must run JavaScript before scraping. To resolve our JavaScript issue, there's a few things we can do. Here, we'll request that the page load, wait one second, and then we're going to grab the source html from the page.\n",
    "\n",
    "Let's continue with Selenium (a headless browser that allows us to render JavaScript just as a human-navigated browser would). The page should believe we're visiting from a live connection on a browser client and the JavaScript should render to be a part of the page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Let's visit our relevant page #\n",
    "driver = webdriver.Firefox()\n",
    "driver.get('http://www.opentable.com/washington-dc-restaurant-listings')\n",
    "\n",
    "# Let's wait one second #\n",
    "sleep(1)\n",
    "\n",
    "# Let's grab the page source #\n",
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Let's convert into a soup object so we can parse it #\n",
    "html = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"booking\"><span class=\"tadpole\"></span> Booked 4 times today</div>,\n",
       " <div class=\"booking\"><span class=\"tadpole\"></span> Booked 1 times today</div>,\n",
       " <div class=\"booking\"><span class=\"tadpole\"></span> Booked 1 times today</div>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the number of times each restaurant was booked again #\n",
    "html.find_all('div', {'class':'booking'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Booked 4 times today\n",
      " Booked 1 times today\n",
      " Booked 1 times today\n"
     ]
    }
   ],
   "source": [
    "# Let's print out the number of times each restaurant was booked today #\n",
    "for entry in html.find_all('div', {'class':'booking'}):\n",
    "    print(entry.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Let's close our driver #\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Note: This notebook was created during the Covid-19 pandemic (resulting in the sparse amount of bookings as seen above).\n",
    "\n",
    "Let's clean this up a little bit. We're going to use Regular Expressions (Regex) to grab only the digits that are available in each of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Let's grab the text for each entry #\n",
    "for booking in html.find_all('div', {'class':'booking'}):\n",
    "    \n",
    "    # Let's match all digits #\n",
    "    match = re.search(r'\\d+', booking.text)\n",
    "    \n",
    "    if match:\n",
    "        print(match.group())\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Sometimes an API doesn't provide all the information we would like to get. Let's continue by using a combination of scraping and API calls to find the ratings and networks of famous television shows. The Internet Movie Database contains data about movies and TV shows. Unfortunately, it does not have a public API. However, the webpage http://www.imdb.com/chart/toptv/?ref_=nv_tp_tv250_2 contains a list of the top 250 tv shows of all time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Let's create a function to obtain the list of the top 250 results #\n",
    "def get_top_250():\n",
    "    response = requests.get('http://www.imdb.com/chart/toptv/?ref_=nv_tp_tv250_2')\n",
    "    html = response.text\n",
    "    \n",
    "    # Let's find everything after title to the next backslash in the a href element #\n",
    "    entries = re.findall('<a href.*?/title/(.*?)/', html)\n",
    "    \n",
    "    # Let's create a list of the top 250 results #\n",
    "    return list(set(entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the function and check the length of the list #\n",
    "entries = get_top_250()\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tt0994314',\n",
       " 'tt0423731',\n",
       " 'tt5788792',\n",
       " 'tt0121955',\n",
       " 'tt5712554',\n",
       " 'tt0088509',\n",
       " 'tt6769208',\n",
       " 'tt0090509',\n",
       " 'tt7651892',\n",
       " 'tt9566030',\n",
       " 'tt0086831',\n",
       " 'tt0417299',\n",
       " 'tt7278862',\n",
       " 'tt0108855',\n",
       " 'tt1910272',\n",
       " 'tt0318871',\n",
       " 'tt0088484',\n",
       " 'tt1533395',\n",
       " 'tt0112130',\n",
       " 'tt1298820',\n",
       " 'tt2395695',\n",
       " 'tt1758429',\n",
       " 'tt5425186',\n",
       " 'tt0275137',\n",
       " 'tt1242773',\n",
       " 'tt4063800',\n",
       " 'tt0758745',\n",
       " 'tt0111893',\n",
       " 'tt4834232',\n",
       " 'tt4158110',\n",
       " 'tt4269716',\n",
       " 'tt2243973',\n",
       " 'tt0074006',\n",
       " 'tt0092337',\n",
       " 'tt4295140',\n",
       " 'tt6077448',\n",
       " 'tt0278238',\n",
       " 'tt0979432',\n",
       " 'tt0214341',\n",
       " 'tt7660850',\n",
       " 'tt1586680',\n",
       " 'tt4574334',\n",
       " 'tt0388629',\n",
       " 'tt0047708',\n",
       " 'tt1733785',\n",
       " 'tt0387764',\n",
       " 'tt0248654',\n",
       " 'tt10233448',\n",
       " 'tt0098936',\n",
       " 'tt5189670',\n",
       " 'tt9561862',\n",
       " 'tt7259746',\n",
       " 'tt0052520',\n",
       " 'tt0795176',\n",
       " 'tt1442449',\n",
       " 'tt6108262',\n",
       " 'tt0081834',\n",
       " 'tt0290978',\n",
       " 'tt0081846',\n",
       " 'tt5555260',\n",
       " 'tt1486217',\n",
       " 'tt9621106',\n",
       " 'tt1305826',\n",
       " 'tt2937900',\n",
       " 'tt0187664',\n",
       " 'tt2306299',\n",
       " 'tt0193676',\n",
       " 'tt4508902',\n",
       " 'tt0075537',\n",
       " 'tt0086661',\n",
       " 'tt2802850',\n",
       " 'tt0403778',\n",
       " 'tt0348914',\n",
       " 'tt0096639',\n",
       " 'tt2571774',\n",
       " 'tt1883092',\n",
       " 'tt0306414',\n",
       " 'tt1494191',\n",
       " 'tt0264235',\n",
       " 'tt1628033',\n",
       " 'tt0314979',\n",
       " 'tt2360717',\n",
       " 'tt0200276',\n",
       " 'tt0129690',\n",
       " 'tt1795096',\n",
       " 'tt5288312',\n",
       " 'tt0988818',\n",
       " 'tt3032476',\n",
       " 'tt6958022',\n",
       " 'tt2575988',\n",
       " 'tt0163507',\n",
       " 'tt1355642',\n",
       " 'tt0096697',\n",
       " 'tt0094517',\n",
       " 'tt0080306',\n",
       " 'tt1474684',\n",
       " 'tt0903747',\n",
       " 'tt9544034',\n",
       " 'tt5249462',\n",
       " 'tt2049116',\n",
       " 'tt0384766',\n",
       " 'tt0380136',\n",
       " 'tt0121220',\n",
       " 'tt0053488',\n",
       " 'tt4786824',\n",
       " 'tt1518542',\n",
       " 'tt0995832',\n",
       " 'tt1492966',\n",
       " 'tt1489428',\n",
       " 'tt0315008',\n",
       " 'tt1190634',\n",
       " 'tt3895150',\n",
       " 'tt0062550',\n",
       " 'tt0386676',\n",
       " 'tt0367279',\n",
       " 'tt4647692',\n",
       " 'tt2560140',\n",
       " 'tt3920596',\n",
       " 'tt0213338',\n",
       " 'tt4742876',\n",
       " 'tt2401256',\n",
       " 'tt8788458',\n",
       " 'tt0112159',\n",
       " 'tt10850932',\n",
       " 'tt1870479',\n",
       " 'tt0472954',\n",
       " 'tt6905542',\n",
       " 'tt3718778',\n",
       " 'tt5421602',\n",
       " 'tt0063929',\n",
       " 'tt3322312',\n",
       " 'tt0111958',\n",
       " 'tt0487831',\n",
       " 'tt9335498',\n",
       " 'tt2433738',\n",
       " 'tt0062588',\n",
       " 'tt2701582',\n",
       " 'tt9253866',\n",
       " 'tt7920978',\n",
       " 'tt0296310',\n",
       " 'tt2356777',\n",
       " 'tt0158417',\n",
       " 'tt0092455',\n",
       " 'tt5687612',\n",
       " 'tt0436992',\n",
       " 'tt4299972',\n",
       " 'tt1513168',\n",
       " 'tt0203082',\n",
       " 'tt2861424',\n",
       " 'tt0081912',\n",
       " 'tt2092588',\n",
       " 'tt1534360',\n",
       " 'tt1508238',\n",
       " 'tt6025022',\n",
       " 'tt0472027',\n",
       " 'tt8420184',\n",
       " 'tt1806234',\n",
       " 'tt3398228',\n",
       " 'tt0475784',\n",
       " 'tt0280249',\n",
       " 'tt3398540',\n",
       " 'tt1606375',\n",
       " 'tt9059760',\n",
       " 'tt5491994',\n",
       " 'tt0106028',\n",
       " 'tt1831164',\n",
       " 'tt5099020',\n",
       " 'tt0098769',\n",
       " 'tt8595766',\n",
       " 'tt0407362',\n",
       " 'tt0185906',\n",
       " 'tt8111088',\n",
       " 'tt0268093',\n",
       " 'tt2303687',\n",
       " 'tt0182629',\n",
       " 'tt9432978',\n",
       " 'tt8289930',\n",
       " 'tt0944947',\n",
       " 'tt0118273',\n",
       " 'tt0118421',\n",
       " 'tt0072500',\n",
       " 'tt5753856',\n",
       " 'tt4093826',\n",
       " 'tt0281491',\n",
       " 'tt0098904',\n",
       " 'tt0106179',\n",
       " 'tt5994364',\n",
       " 'tt1832668',\n",
       " 'tt0096657',\n",
       " 'tt5675620',\n",
       " 'tt0988824',\n",
       " 'tt7562112',\n",
       " 'tt4288182',\n",
       " 'tt0286486',\n",
       " 'tt0434706',\n",
       " 'tt1865718',\n",
       " 'tt0264700',\n",
       " 'tt0092324',\n",
       " 'tt0807832',\n",
       " 'tt1043822',\n",
       " 'tt0353049',\n",
       " 'tt0459159',\n",
       " 'tt0804503',\n",
       " 'tt2085059',\n",
       " 'tt3530232',\n",
       " 'tt1266020',\n",
       " 'tt2244495',\n",
       " 'tt1124373',\n",
       " 'tt0229889',\n",
       " 'tt0346314',\n",
       " 'tt1439629',\n",
       " 'tt0108778',\n",
       " 'tt12004706',\n",
       " 'tt0417349',\n",
       " 'tt1856010',\n",
       " 'tt7137906',\n",
       " 'tt0863046',\n",
       " 'tt0262150',\n",
       " 'tt5897304',\n",
       " 'tt1475582',\n",
       " 'tt2707408',\n",
       " 'tt7366338',\n",
       " 'tt0071075',\n",
       " 'tt6763664',\n",
       " 'tt1641384',\n",
       " 'tt4082744',\n",
       " 'tt1984119',\n",
       " 'tt2442560',\n",
       " 'tt0877057',\n",
       " 'tt0773262',\n",
       " 'tt0103359',\n",
       " 'tt0412142',\n",
       " 'tt0118266',\n",
       " 'tt0417373',\n",
       " 'tt0096548',\n",
       " 'tt5290382',\n",
       " 'tt0237123',\n",
       " '?count=100&amp;groups=oscar_best_picture_winners&amp;sort=year%2Cdesc&amp;ref_=nv_ch_osc\" tabindex=\"-1\" aria-disabled=\"false\"><span class=\"ipc-list-item__text\" role=\"presentation\">Best Picture Winners<',\n",
       " 'tt5626028',\n",
       " 'tt7221388',\n",
       " 'tt0094525',\n",
       " 'tt0141842',\n",
       " 'tt6111552',\n",
       " 'tt1227926',\n",
       " 'tt0421357',\n",
       " 'tt2098220',\n",
       " 'tt2297757',\n",
       " 'tt2100976',\n",
       " 'tt0120570',\n",
       " 'tt1877514',\n",
       " 'tt0303461']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out why we get an extra entry in our list #\n",
    "entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?count=100&amp;groups=oscar_best_picture_winners&amp;sort=year%2Cdesc&amp;ref_=nv_ch_osc\" tabindex=\"-1\" aria-disabled=\"false\"><span class=\"ipc-list-item__text\" role=\"presentation\">Best Picture Winners<'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a loop to find the index value of the entry we don't need and drop it from the list #\n",
    "nn = 0\n",
    "\n",
    "for x in range(len(entries)):\n",
    "    if 'tt' not in entries[x]:\n",
    "        nn = x\n",
    "    else: pass\n",
    "\n",
    "entries.pop(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the length of the list again #\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Although the Internet Movie Database does not have a public API, an open API exists at http://www.tvmaze.com/api. Let's use this API to retrieve information about each of the 250 TV shows we have just extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull information from the API using Json interaction and store into a DataFrame #\n",
    "shows_df1= pd.DataFrame(columns = ['show_name', 'rating_avg', 'genres', 'network', 'premiere_date', 'status'])\n",
    "\n",
    "def get_entry(entry):\n",
    "    res=requests.get('http://api.tvmaze.com/lookup/shows?imdb='+entry)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        try:\n",
    "            status = json.loads(res.text).get('status')\n",
    "        except AttributeError:\n",
    "            status = 'NA'\n",
    "        try: \n",
    "            rating = json.loads(res.text).get('rating').get('average')\n",
    "        except AttributeError:\n",
    "            rating = 'NA'\n",
    "            \n",
    "        try:\n",
    "            network = json.loads(res.text).get('network').get('name')\n",
    "        except AttributeError:\n",
    "            network = 'NA'\n",
    "            \n",
    "        try:\n",
    "            title = json.loads(res.text).get('name')\n",
    "        except AttributeError:\n",
    "            title = 'NA'\n",
    "            \n",
    "        try:\n",
    "            premier = json.loads(res.text).get('premiered')\n",
    "        except AttributeError:\n",
    "            premier = 'NA'\n",
    "            \n",
    "        try:\n",
    "            genres = json.loads(res.text).get('genres')\n",
    "        except AttributeError:\n",
    "            genres = 'NA'\n",
    "\n",
    "       \n",
    "        shows_df1.loc[len(shows_df1)] = [title, rating, genres, network, premier, status]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_name</th>\n",
       "      <th>rating_avg</th>\n",
       "      <th>genres</th>\n",
       "      <th>network</th>\n",
       "      <th>premiere_date</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Code Geass</td>\n",
       "      <td>8.1</td>\n",
       "      <td>[Drama, Action, Anime, Science-Fiction]</td>\n",
       "      <td>MBS</td>\n",
       "      <td>2006-10-05</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samurai Champloo</td>\n",
       "      <td>7.7</td>\n",
       "      <td>[Comedy, Action, Adventure, Anime]</td>\n",
       "      <td>Fuji TV</td>\n",
       "      <td>2004-05-20</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Marvelous Mrs. Maisel</td>\n",
       "      <td>8.4</td>\n",
       "      <td>[Drama, Comedy]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2017-03-17</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Park</td>\n",
       "      <td>8.6</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Comedy Central</td>\n",
       "      <td>1997-08-13</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Grand Tour</td>\n",
       "      <td>8.2</td>\n",
       "      <td>[Comedy, Adventure]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2016-11-18</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Blue Planet II</td>\n",
       "      <td>9.5</td>\n",
       "      <td>[Nature]</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>2017-10-29</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>One Strange Rock</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[]</td>\n",
       "      <td>National Geographic Channel</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Yes, Prime Minister</td>\n",
       "      <td>8.6</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>BBC Two</td>\n",
       "      <td>1986-01-09</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Avatar: The Last Airbender</td>\n",
       "      <td>8.9</td>\n",
       "      <td>[Action, Adventure, Fantasy]</td>\n",
       "      <td>Nickelodeon</td>\n",
       "      <td>2005-02-21</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>My Brilliant Friend</td>\n",
       "      <td>7.6</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>Rai 1</td>\n",
       "      <td>2018-11-18</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Steins Gate</td>\n",
       "      <td>8.8</td>\n",
       "      <td>[Drama, Anime, Fantasy, Science-Fiction]</td>\n",
       "      <td>SUN-TV</td>\n",
       "      <td>2011-04-06</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Berserk</td>\n",
       "      <td>8.8</td>\n",
       "      <td>[Anime, Fantasy, Horror]</td>\n",
       "      <td>NTV</td>\n",
       "      <td>1997-10-07</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Life</td>\n",
       "      <td>7.4</td>\n",
       "      <td>[Nature]</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>2009-10-12</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[Drama, Romance]</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>1995-09-24</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cosmos</td>\n",
       "      <td>8.9</td>\n",
       "      <td>[]</td>\n",
       "      <td>National Geographic Channel</td>\n",
       "      <td>1980-09-28</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Spartacus: Gods of the Arena</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[Action, Adventure]</td>\n",
       "      <td>Starz</td>\n",
       "      <td>2011-01-21</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Horace and Pete</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[Drama, Comedy]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2016-01-30</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Justice League Unlimited</td>\n",
       "      <td>8.8</td>\n",
       "      <td>[Action, Adventure, Science-Fiction]</td>\n",
       "      <td>Cartoon Network</td>\n",
       "      <td>2004-07-31</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Romanzo Criminale</td>\n",
       "      <td>7.8</td>\n",
       "      <td>[Drama, Crime, Thriller]</td>\n",
       "      <td>Sky Cinema</td>\n",
       "      <td>2008-11-10</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Le Bureau des Lgendes</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>Canal+</td>\n",
       "      <td>2015-04-27</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Friday Night Lights</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[Drama, Sports]</td>\n",
       "      <td>NBC</td>\n",
       "      <td>2006-10-03</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Beatles Anthology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1995-11-11</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Critical Role</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Adventure, Fantasy]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2015-03-13</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mr. Robot</td>\n",
       "      <td>8.4</td>\n",
       "      <td>[Drama, Crime, Thriller]</td>\n",
       "      <td>USA Network</td>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Umbre</td>\n",
       "      <td>7.8</td>\n",
       "      <td>[Drama, Crime]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2014-12-28</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hannibal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>[Drama, Crime, Horror]</td>\n",
       "      <td>NBC</td>\n",
       "      <td>2013-04-04</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I, Claudius</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[Drama, History]</td>\n",
       "      <td>BBC Two</td>\n",
       "      <td>1976-09-20</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Dekalog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>TVP1</td>\n",
       "      <td>1989-12-10</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chef's Table</td>\n",
       "      <td>8.1</td>\n",
       "      <td>[Food]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2015-04-26</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sacred Games</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[Drama, Action, Crime]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2018-07-06</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Black Books</td>\n",
       "      <td>8.6</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Channel 4</td>\n",
       "      <td>2000-09-29</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Sherlock</td>\n",
       "      <td>9.1</td>\n",
       "      <td>[Drama, Crime, Mystery]</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>2010-07-25</td>\n",
       "      <td>To Be Determined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Narcos</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Drama, Action, Crime]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Chernobyl</td>\n",
       "      <td>9.2</td>\n",
       "      <td>[Drama, History]</td>\n",
       "      <td>HBO</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>The World at War</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[War, History]</td>\n",
       "      <td>ITV</td>\n",
       "      <td>1973-10-31</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>8.3</td>\n",
       "      <td>[Drama, Horror, Thriller]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Young Justice</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Action, Adventure, Science-Fiction]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2010-11-26</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Detectorists</td>\n",
       "      <td>8.8</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>BBC Four</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Feud</td>\n",
       "      <td>7.6</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>FX</td>\n",
       "      <td>2017-03-05</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Peaky Blinders</td>\n",
       "      <td>8.8</td>\n",
       "      <td>[Drama, Crime, History]</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>2013-09-12</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Death Note</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[Drama, Anime, Thriller, Mystery]</td>\n",
       "      <td>NTV</td>\n",
       "      <td>2006-10-03</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Dexter</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Drama, Crime, Mystery]</td>\n",
       "      <td>Showtime</td>\n",
       "      <td>2006-10-01</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Batman: The Animated Series</td>\n",
       "      <td>8.9</td>\n",
       "      <td>[Action, Adventure, Science-Fiction]</td>\n",
       "      <td>FOX</td>\n",
       "      <td>1992-09-05</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>House</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[Drama, Mystery, Medical]</td>\n",
       "      <td>FOX</td>\n",
       "      <td>2004-11-16</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>The New Batman Adventures</td>\n",
       "      <td>8.6</td>\n",
       "      <td>[Action, Adventure, Science-Fiction]</td>\n",
       "      <td>The WB</td>\n",
       "      <td>1997-09-13</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>The Venture Bros.</td>\n",
       "      <td>8.9</td>\n",
       "      <td>[Comedy, Adventure]</td>\n",
       "      <td>Adult Swim</td>\n",
       "      <td>2004-08-07</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>MINDHUNTER</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Drama, Crime, Thriller]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2017-10-13</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Coupling</td>\n",
       "      <td>8.6</td>\n",
       "      <td>[Comedy, Romance]</td>\n",
       "      <td>BBC Three</td>\n",
       "      <td>2000-05-12</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Boku no Hero Academia</td>\n",
       "      <td>8.3</td>\n",
       "      <td>[Comedy, Action, Adventure, Anime]</td>\n",
       "      <td>MBS</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Cobra Kai</td>\n",
       "      <td>6.9</td>\n",
       "      <td>[Drama, Comedy, Action]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2018-05-02</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Agatha Christie's Poirot</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Drama, Crime, Mystery]</td>\n",
       "      <td>ITV</td>\n",
       "      <td>1989-01-08</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>The Sopranos</td>\n",
       "      <td>9.1</td>\n",
       "      <td>[Drama, Crime]</td>\n",
       "      <td>HBO</td>\n",
       "      <td>1999-01-10</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Dr. Horrible's Sing-Along Blog</td>\n",
       "      <td>8.3</td>\n",
       "      <td>[Comedy, Music, Science-Fiction]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2008-07-15</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Fullmetal Alchemist</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Action, Adventure, Anime]</td>\n",
       "      <td>Animax</td>\n",
       "      <td>2003-10-04</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Hunter x Hunter</td>\n",
       "      <td>8.2</td>\n",
       "      <td>[Action, Adventure, Anime, Fantasy]</td>\n",
       "      <td>NTV</td>\n",
       "      <td>2011-10-02</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Nathan for You</td>\n",
       "      <td>7.1</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Comedy Central</td>\n",
       "      <td>2013-02-28</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Impractical Jokers</td>\n",
       "      <td>7.7</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>truTV</td>\n",
       "      <td>2011-12-15</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>From the Earth to the Moon</td>\n",
       "      <td>7.8</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>HBO</td>\n",
       "      <td>1998-04-05</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>The Vietnam War</td>\n",
       "      <td>7.6</td>\n",
       "      <td>[War, History]</td>\n",
       "      <td>PBS</td>\n",
       "      <td>2017-09-17</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Firefly</td>\n",
       "      <td>9.2</td>\n",
       "      <td>[Drama, Adventure, Science-Fiction]</td>\n",
       "      <td>FOX</td>\n",
       "      <td>2002-09-20</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          show_name  rating_avg  \\\n",
       "0                        Code Geass         8.1   \n",
       "1                  Samurai Champloo         7.7   \n",
       "2         The Marvelous Mrs. Maisel         8.4   \n",
       "3                        South Park         8.6   \n",
       "4                    The Grand Tour         8.2   \n",
       "5                    Blue Planet II         9.5   \n",
       "6                  One Strange Rock         7.5   \n",
       "7               Yes, Prime Minister         8.6   \n",
       "8        Avatar: The Last Airbender         8.9   \n",
       "9               My Brilliant Friend         7.6   \n",
       "10                      Steins Gate         8.8   \n",
       "11                          Berserk         8.8   \n",
       "12                             Life         7.4   \n",
       "13              Pride and Prejudice         8.7   \n",
       "14                           Cosmos         8.9   \n",
       "15     Spartacus: Gods of the Arena         8.7   \n",
       "16                  Horace and Pete         8.0   \n",
       "17         Justice League Unlimited         8.8   \n",
       "18                Romanzo Criminale         7.8   \n",
       "19           Le Bureau des Lgendes         8.5   \n",
       "20              Friday Night Lights         8.7   \n",
       "21            The Beatles Anthology         NaN   \n",
       "22                    Critical Role         NaN   \n",
       "23                        Mr. Robot         8.4   \n",
       "24                            Umbre         7.8   \n",
       "25                         Hannibal         8.4   \n",
       "26                      I, Claudius         8.7   \n",
       "27                          Dekalog         NaN   \n",
       "28                     Chef's Table         8.1   \n",
       "29                     Sacred Games         7.0   \n",
       "..                              ...         ...   \n",
       "196                     Black Books         8.6   \n",
       "197                        Sherlock         9.1   \n",
       "198                          Narcos         8.5   \n",
       "199                       Chernobyl         9.2   \n",
       "200                The World at War         NaN   \n",
       "201      The Haunting of Hill House         8.3   \n",
       "202                   Young Justice         8.5   \n",
       "203                    Detectorists         8.8   \n",
       "204                            Feud         7.6   \n",
       "205                  Peaky Blinders         8.8   \n",
       "206                      Death Note         8.7   \n",
       "207                          Dexter         8.5   \n",
       "208     Batman: The Animated Series         8.9   \n",
       "209                           House         9.0   \n",
       "210       The New Batman Adventures         8.6   \n",
       "211               The Venture Bros.         8.9   \n",
       "212                      MINDHUNTER         8.5   \n",
       "213                        Coupling         8.6   \n",
       "214           Boku no Hero Academia         8.3   \n",
       "215                       Cobra Kai         6.9   \n",
       "216        Agatha Christie's Poirot         8.5   \n",
       "217                    The Sopranos         9.1   \n",
       "218  Dr. Horrible's Sing-Along Blog         8.3   \n",
       "219             Fullmetal Alchemist         8.5   \n",
       "220                 Hunter x Hunter         8.2   \n",
       "221                  Nathan for You         7.1   \n",
       "222              Impractical Jokers         7.7   \n",
       "223      From the Earth to the Moon         7.8   \n",
       "224                 The Vietnam War         7.6   \n",
       "225                         Firefly         9.2   \n",
       "\n",
       "                                       genres                      network  \\\n",
       "0     [Drama, Action, Anime, Science-Fiction]                          MBS   \n",
       "1          [Comedy, Action, Adventure, Anime]                      Fuji TV   \n",
       "2                             [Drama, Comedy]                           NA   \n",
       "3                                    [Comedy]               Comedy Central   \n",
       "4                         [Comedy, Adventure]                           NA   \n",
       "5                                    [Nature]                      BBC One   \n",
       "6                                          []  National Geographic Channel   \n",
       "7                                    [Comedy]                      BBC Two   \n",
       "8                [Action, Adventure, Fantasy]                  Nickelodeon   \n",
       "9                                     [Drama]                        Rai 1   \n",
       "10   [Drama, Anime, Fantasy, Science-Fiction]                       SUN-TV   \n",
       "11                   [Anime, Fantasy, Horror]                          NTV   \n",
       "12                                   [Nature]                      BBC One   \n",
       "13                           [Drama, Romance]                      BBC One   \n",
       "14                                         []  National Geographic Channel   \n",
       "15                        [Action, Adventure]                        Starz   \n",
       "16                            [Drama, Comedy]                           NA   \n",
       "17       [Action, Adventure, Science-Fiction]              Cartoon Network   \n",
       "18                   [Drama, Crime, Thriller]                   Sky Cinema   \n",
       "19                                    [Drama]                       Canal+   \n",
       "20                            [Drama, Sports]                          NBC   \n",
       "21                                    [Music]                          ABC   \n",
       "22                       [Adventure, Fantasy]                           NA   \n",
       "23                   [Drama, Crime, Thriller]                  USA Network   \n",
       "24                             [Drama, Crime]                           NA   \n",
       "25                     [Drama, Crime, Horror]                          NBC   \n",
       "26                           [Drama, History]                      BBC Two   \n",
       "27                                    [Drama]                         TVP1   \n",
       "28                                     [Food]                           NA   \n",
       "29                     [Drama, Action, Crime]                           NA   \n",
       "..                                        ...                          ...   \n",
       "196                                  [Comedy]                    Channel 4   \n",
       "197                   [Drama, Crime, Mystery]                      BBC One   \n",
       "198                    [Drama, Action, Crime]                           NA   \n",
       "199                          [Drama, History]                          HBO   \n",
       "200                            [War, History]                          ITV   \n",
       "201                 [Drama, Horror, Thriller]                           NA   \n",
       "202      [Action, Adventure, Science-Fiction]                           NA   \n",
       "203                                  [Comedy]                     BBC Four   \n",
       "204                                   [Drama]                           FX   \n",
       "205                   [Drama, Crime, History]                      BBC One   \n",
       "206         [Drama, Anime, Thriller, Mystery]                          NTV   \n",
       "207                   [Drama, Crime, Mystery]                     Showtime   \n",
       "208      [Action, Adventure, Science-Fiction]                          FOX   \n",
       "209                 [Drama, Mystery, Medical]                          FOX   \n",
       "210      [Action, Adventure, Science-Fiction]                       The WB   \n",
       "211                       [Comedy, Adventure]                   Adult Swim   \n",
       "212                  [Drama, Crime, Thriller]                           NA   \n",
       "213                         [Comedy, Romance]                    BBC Three   \n",
       "214        [Comedy, Action, Adventure, Anime]                          MBS   \n",
       "215                   [Drama, Comedy, Action]                           NA   \n",
       "216                   [Drama, Crime, Mystery]                          ITV   \n",
       "217                            [Drama, Crime]                          HBO   \n",
       "218          [Comedy, Music, Science-Fiction]                           NA   \n",
       "219                [Action, Adventure, Anime]                       Animax   \n",
       "220       [Action, Adventure, Anime, Fantasy]                          NTV   \n",
       "221                                  [Comedy]               Comedy Central   \n",
       "222                                  [Comedy]                        truTV   \n",
       "223                                   [Drama]                          HBO   \n",
       "224                            [War, History]                          PBS   \n",
       "225       [Drama, Adventure, Science-Fiction]                          FOX   \n",
       "\n",
       "    premiere_date            status  \n",
       "0      2006-10-05             Ended  \n",
       "1      2004-05-20             Ended  \n",
       "2      2017-03-17           Running  \n",
       "3      1997-08-13           Running  \n",
       "4      2016-11-18           Running  \n",
       "5      2017-10-29             Ended  \n",
       "6      2018-03-26           Running  \n",
       "7      1986-01-09             Ended  \n",
       "8      2005-02-21             Ended  \n",
       "9      2018-11-18           Running  \n",
       "10     2011-04-06             Ended  \n",
       "11     1997-10-07             Ended  \n",
       "12     2009-10-12             Ended  \n",
       "13     1995-09-24             Ended  \n",
       "14     1980-09-28             Ended  \n",
       "15     2011-01-21             Ended  \n",
       "16     2016-01-30             Ended  \n",
       "17     2004-07-31             Ended  \n",
       "18     2008-11-10             Ended  \n",
       "19     2015-04-27           Running  \n",
       "20     2006-10-03             Ended  \n",
       "21     1995-11-11             Ended  \n",
       "22     2015-03-13           Running  \n",
       "23     2015-06-24             Ended  \n",
       "24     2014-12-28           Running  \n",
       "25     2013-04-04             Ended  \n",
       "26     1976-09-20             Ended  \n",
       "27     1989-12-10             Ended  \n",
       "28     2015-04-26           Running  \n",
       "29     2018-07-06           Running  \n",
       "..            ...               ...  \n",
       "196    2000-09-29             Ended  \n",
       "197    2010-07-25  To Be Determined  \n",
       "198    2015-08-28             Ended  \n",
       "199    2019-05-06             Ended  \n",
       "200    1973-10-31             Ended  \n",
       "201    2018-10-12           Running  \n",
       "202    2010-11-26           Running  \n",
       "203    2014-10-02             Ended  \n",
       "204    2017-03-05             Ended  \n",
       "205    2013-09-12           Running  \n",
       "206    2006-10-03             Ended  \n",
       "207    2006-10-01             Ended  \n",
       "208    1992-09-05             Ended  \n",
       "209    2004-11-16             Ended  \n",
       "210    1997-09-13             Ended  \n",
       "211    2004-08-07           Running  \n",
       "212    2017-10-13             Ended  \n",
       "213    2000-05-12             Ended  \n",
       "214    2016-04-03           Running  \n",
       "215    2018-05-02           Running  \n",
       "216    1989-01-08             Ended  \n",
       "217    1999-01-10             Ended  \n",
       "218    2008-07-15             Ended  \n",
       "219    2003-10-04             Ended  \n",
       "220    2011-10-02             Ended  \n",
       "221    2013-02-28             Ended  \n",
       "222    2011-12-15           Running  \n",
       "223    1998-04-05             Ended  \n",
       "224    2017-09-17             Ended  \n",
       "225    2002-09-20             Ended  \n",
       "\n",
       "[226 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the above function #\n",
    "for entry in entries:\n",
    "    get_entry(entry)\n",
    "    \n",
    "shows_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 226 entries, 0 to 225\n",
      "Data columns (total 6 columns):\n",
      "show_name        226 non-null object\n",
      "rating_avg       203 non-null float64\n",
      "genres           226 non-null object\n",
      "network          226 non-null object\n",
      "premiere_date    226 non-null object\n",
      "status           226 non-null object\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 12.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Let's check for null values present, as well as data types for each column #\n",
    "shows_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull information from the API converting Json into a python dictionary element #\n",
    "shows_df2= pd.DataFrame(columns = ['show_name', 'rating_avg', 'genres', 'network', 'premiere_date', 'status'])\n",
    "\n",
    "def get_entry(entry):\n",
    "    res=requests.get('http://api.tvmaze.com/lookup/shows?imdb='+entry)\n",
    "    if res.status_code == 200:\n",
    "        results = json.loads(res.text)\n",
    "        \n",
    "        try:    \n",
    "            status = results['status']\n",
    "        except TypeError:\n",
    "            status = 'NA'   \n",
    "        try:\n",
    "            rating = results['rating']['average']\n",
    "        except TypeError:\n",
    "            rating = 'NA'\n",
    "        try:\n",
    "            network = results['network']['name']\n",
    "        except TypeError:\n",
    "            network = 'NA'\n",
    "        try:   \n",
    "            title = results['name']\n",
    "        except TypeError:\n",
    "            title = 'NA'\n",
    "        try:   \n",
    "            genres = results['genres']\n",
    "        except TypeError:\n",
    "            genres = 'NA'\n",
    "        try:   \n",
    "            premier = results['premiered']\n",
    "        except TypeError:\n",
    "            premier = 'NA'\n",
    "        \n",
    "        shows_df2.loc[len(shows_df2)] = [title, rating, genres, network, premier, status]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_name</th>\n",
       "      <th>rating_avg</th>\n",
       "      <th>genres</th>\n",
       "      <th>network</th>\n",
       "      <th>premiere_date</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Code Geass</td>\n",
       "      <td>8.1</td>\n",
       "      <td>[Drama, Action, Anime, Science-Fiction]</td>\n",
       "      <td>MBS</td>\n",
       "      <td>2006-10-05</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samurai Champloo</td>\n",
       "      <td>7.7</td>\n",
       "      <td>[Comedy, Action, Adventure, Anime]</td>\n",
       "      <td>Fuji TV</td>\n",
       "      <td>2004-05-20</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Marvelous Mrs. Maisel</td>\n",
       "      <td>8.4</td>\n",
       "      <td>[Drama, Comedy]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2017-03-17</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Park</td>\n",
       "      <td>8.6</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Comedy Central</td>\n",
       "      <td>1997-08-13</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Grand Tour</td>\n",
       "      <td>8.2</td>\n",
       "      <td>[Comedy, Adventure]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2016-11-18</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Blue Planet II</td>\n",
       "      <td>9.5</td>\n",
       "      <td>[Nature]</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>2017-10-29</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>One Strange Rock</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[]</td>\n",
       "      <td>National Geographic Channel</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Yes, Prime Minister</td>\n",
       "      <td>8.6</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>BBC Two</td>\n",
       "      <td>1986-01-09</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Avatar: The Last Airbender</td>\n",
       "      <td>8.9</td>\n",
       "      <td>[Action, Adventure, Fantasy]</td>\n",
       "      <td>Nickelodeon</td>\n",
       "      <td>2005-02-21</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>My Brilliant Friend</td>\n",
       "      <td>7.6</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>Rai 1</td>\n",
       "      <td>2018-11-18</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Steins Gate</td>\n",
       "      <td>8.8</td>\n",
       "      <td>[Drama, Anime, Fantasy, Science-Fiction]</td>\n",
       "      <td>SUN-TV</td>\n",
       "      <td>2011-04-06</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Berserk</td>\n",
       "      <td>8.8</td>\n",
       "      <td>[Anime, Fantasy, Horror]</td>\n",
       "      <td>NTV</td>\n",
       "      <td>1997-10-07</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Life</td>\n",
       "      <td>7.4</td>\n",
       "      <td>[Nature]</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>2009-10-12</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[Drama, Romance]</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>1995-09-24</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cosmos</td>\n",
       "      <td>8.9</td>\n",
       "      <td>[]</td>\n",
       "      <td>National Geographic Channel</td>\n",
       "      <td>1980-09-28</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Spartacus: Gods of the Arena</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[Action, Adventure]</td>\n",
       "      <td>Starz</td>\n",
       "      <td>2011-01-21</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Horace and Pete</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[Drama, Comedy]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2016-01-30</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Justice League Unlimited</td>\n",
       "      <td>8.8</td>\n",
       "      <td>[Action, Adventure, Science-Fiction]</td>\n",
       "      <td>Cartoon Network</td>\n",
       "      <td>2004-07-31</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Romanzo Criminale</td>\n",
       "      <td>7.8</td>\n",
       "      <td>[Drama, Crime, Thriller]</td>\n",
       "      <td>Sky Cinema</td>\n",
       "      <td>2008-11-10</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Le Bureau des Lgendes</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>Canal+</td>\n",
       "      <td>2015-04-27</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Friday Night Lights</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[Drama, Sports]</td>\n",
       "      <td>NBC</td>\n",
       "      <td>2006-10-03</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Beatles Anthology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>ABC</td>\n",
       "      <td>1995-11-11</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Critical Role</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Adventure, Fantasy]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2015-03-13</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mr. Robot</td>\n",
       "      <td>8.4</td>\n",
       "      <td>[Drama, Crime, Thriller]</td>\n",
       "      <td>USA Network</td>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Umbre</td>\n",
       "      <td>7.8</td>\n",
       "      <td>[Drama, Crime]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2014-12-28</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hannibal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>[Drama, Crime, Horror]</td>\n",
       "      <td>NBC</td>\n",
       "      <td>2013-04-04</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I, Claudius</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[Drama, History]</td>\n",
       "      <td>BBC Two</td>\n",
       "      <td>1976-09-20</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Dekalog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>TVP1</td>\n",
       "      <td>1989-12-10</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chef's Table</td>\n",
       "      <td>8.1</td>\n",
       "      <td>[Food]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2015-04-26</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sacred Games</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[Drama, Action, Crime]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2018-07-06</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Black Books</td>\n",
       "      <td>8.6</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Channel 4</td>\n",
       "      <td>2000-09-29</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Sherlock</td>\n",
       "      <td>9.1</td>\n",
       "      <td>[Drama, Crime, Mystery]</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>2010-07-25</td>\n",
       "      <td>To Be Determined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Narcos</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Drama, Action, Crime]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Chernobyl</td>\n",
       "      <td>9.2</td>\n",
       "      <td>[Drama, History]</td>\n",
       "      <td>HBO</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>The World at War</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[War, History]</td>\n",
       "      <td>ITV</td>\n",
       "      <td>1973-10-31</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>8.3</td>\n",
       "      <td>[Drama, Horror, Thriller]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2018-10-12</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Young Justice</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Action, Adventure, Science-Fiction]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2010-11-26</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Detectorists</td>\n",
       "      <td>8.8</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>BBC Four</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Feud</td>\n",
       "      <td>7.6</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>FX</td>\n",
       "      <td>2017-03-05</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Peaky Blinders</td>\n",
       "      <td>8.8</td>\n",
       "      <td>[Drama, Crime, History]</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>2013-09-12</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Death Note</td>\n",
       "      <td>8.7</td>\n",
       "      <td>[Drama, Anime, Thriller, Mystery]</td>\n",
       "      <td>NTV</td>\n",
       "      <td>2006-10-03</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Dexter</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Drama, Crime, Mystery]</td>\n",
       "      <td>Showtime</td>\n",
       "      <td>2006-10-01</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Batman: The Animated Series</td>\n",
       "      <td>8.9</td>\n",
       "      <td>[Action, Adventure, Science-Fiction]</td>\n",
       "      <td>FOX</td>\n",
       "      <td>1992-09-05</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>House</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[Drama, Mystery, Medical]</td>\n",
       "      <td>FOX</td>\n",
       "      <td>2004-11-16</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>The New Batman Adventures</td>\n",
       "      <td>8.6</td>\n",
       "      <td>[Action, Adventure, Science-Fiction]</td>\n",
       "      <td>The WB</td>\n",
       "      <td>1997-09-13</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>The Venture Bros.</td>\n",
       "      <td>8.9</td>\n",
       "      <td>[Comedy, Adventure]</td>\n",
       "      <td>Adult Swim</td>\n",
       "      <td>2004-08-07</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>MINDHUNTER</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Drama, Crime, Thriller]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2017-10-13</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Coupling</td>\n",
       "      <td>8.6</td>\n",
       "      <td>[Comedy, Romance]</td>\n",
       "      <td>BBC Three</td>\n",
       "      <td>2000-05-12</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Boku no Hero Academia</td>\n",
       "      <td>8.3</td>\n",
       "      <td>[Comedy, Action, Adventure, Anime]</td>\n",
       "      <td>MBS</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Cobra Kai</td>\n",
       "      <td>6.9</td>\n",
       "      <td>[Drama, Comedy, Action]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2018-05-02</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Agatha Christie's Poirot</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Drama, Crime, Mystery]</td>\n",
       "      <td>ITV</td>\n",
       "      <td>1989-01-08</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>The Sopranos</td>\n",
       "      <td>9.1</td>\n",
       "      <td>[Drama, Crime]</td>\n",
       "      <td>HBO</td>\n",
       "      <td>1999-01-10</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Dr. Horrible's Sing-Along Blog</td>\n",
       "      <td>8.3</td>\n",
       "      <td>[Comedy, Music, Science-Fiction]</td>\n",
       "      <td>NA</td>\n",
       "      <td>2008-07-15</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Fullmetal Alchemist</td>\n",
       "      <td>8.5</td>\n",
       "      <td>[Action, Adventure, Anime]</td>\n",
       "      <td>Animax</td>\n",
       "      <td>2003-10-04</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Hunter x Hunter</td>\n",
       "      <td>8.2</td>\n",
       "      <td>[Action, Adventure, Anime, Fantasy]</td>\n",
       "      <td>NTV</td>\n",
       "      <td>2011-10-02</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Nathan for You</td>\n",
       "      <td>7.1</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>Comedy Central</td>\n",
       "      <td>2013-02-28</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Impractical Jokers</td>\n",
       "      <td>7.7</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>truTV</td>\n",
       "      <td>2011-12-15</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>From the Earth to the Moon</td>\n",
       "      <td>7.8</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>HBO</td>\n",
       "      <td>1998-04-05</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>The Vietnam War</td>\n",
       "      <td>7.6</td>\n",
       "      <td>[War, History]</td>\n",
       "      <td>PBS</td>\n",
       "      <td>2017-09-17</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Firefly</td>\n",
       "      <td>9.2</td>\n",
       "      <td>[Drama, Adventure, Science-Fiction]</td>\n",
       "      <td>FOX</td>\n",
       "      <td>2002-09-20</td>\n",
       "      <td>Ended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          show_name  rating_avg  \\\n",
       "0                        Code Geass         8.1   \n",
       "1                  Samurai Champloo         7.7   \n",
       "2         The Marvelous Mrs. Maisel         8.4   \n",
       "3                        South Park         8.6   \n",
       "4                    The Grand Tour         8.2   \n",
       "5                    Blue Planet II         9.5   \n",
       "6                  One Strange Rock         7.5   \n",
       "7               Yes, Prime Minister         8.6   \n",
       "8        Avatar: The Last Airbender         8.9   \n",
       "9               My Brilliant Friend         7.6   \n",
       "10                      Steins Gate         8.8   \n",
       "11                          Berserk         8.8   \n",
       "12                             Life         7.4   \n",
       "13              Pride and Prejudice         8.7   \n",
       "14                           Cosmos         8.9   \n",
       "15     Spartacus: Gods of the Arena         8.7   \n",
       "16                  Horace and Pete         8.0   \n",
       "17         Justice League Unlimited         8.8   \n",
       "18                Romanzo Criminale         7.8   \n",
       "19           Le Bureau des Lgendes         8.5   \n",
       "20              Friday Night Lights         8.7   \n",
       "21            The Beatles Anthology         NaN   \n",
       "22                    Critical Role         NaN   \n",
       "23                        Mr. Robot         8.4   \n",
       "24                            Umbre         7.8   \n",
       "25                         Hannibal         8.4   \n",
       "26                      I, Claudius         8.7   \n",
       "27                          Dekalog         NaN   \n",
       "28                     Chef's Table         8.1   \n",
       "29                     Sacred Games         7.0   \n",
       "..                              ...         ...   \n",
       "196                     Black Books         8.6   \n",
       "197                        Sherlock         9.1   \n",
       "198                          Narcos         8.5   \n",
       "199                       Chernobyl         9.2   \n",
       "200                The World at War         NaN   \n",
       "201      The Haunting of Hill House         8.3   \n",
       "202                   Young Justice         8.5   \n",
       "203                    Detectorists         8.8   \n",
       "204                            Feud         7.6   \n",
       "205                  Peaky Blinders         8.8   \n",
       "206                      Death Note         8.7   \n",
       "207                          Dexter         8.5   \n",
       "208     Batman: The Animated Series         8.9   \n",
       "209                           House         9.0   \n",
       "210       The New Batman Adventures         8.6   \n",
       "211               The Venture Bros.         8.9   \n",
       "212                      MINDHUNTER         8.5   \n",
       "213                        Coupling         8.6   \n",
       "214           Boku no Hero Academia         8.3   \n",
       "215                       Cobra Kai         6.9   \n",
       "216        Agatha Christie's Poirot         8.5   \n",
       "217                    The Sopranos         9.1   \n",
       "218  Dr. Horrible's Sing-Along Blog         8.3   \n",
       "219             Fullmetal Alchemist         8.5   \n",
       "220                 Hunter x Hunter         8.2   \n",
       "221                  Nathan for You         7.1   \n",
       "222              Impractical Jokers         7.7   \n",
       "223      From the Earth to the Moon         7.8   \n",
       "224                 The Vietnam War         7.6   \n",
       "225                         Firefly         9.2   \n",
       "\n",
       "                                       genres                      network  \\\n",
       "0     [Drama, Action, Anime, Science-Fiction]                          MBS   \n",
       "1          [Comedy, Action, Adventure, Anime]                      Fuji TV   \n",
       "2                             [Drama, Comedy]                           NA   \n",
       "3                                    [Comedy]               Comedy Central   \n",
       "4                         [Comedy, Adventure]                           NA   \n",
       "5                                    [Nature]                      BBC One   \n",
       "6                                          []  National Geographic Channel   \n",
       "7                                    [Comedy]                      BBC Two   \n",
       "8                [Action, Adventure, Fantasy]                  Nickelodeon   \n",
       "9                                     [Drama]                        Rai 1   \n",
       "10   [Drama, Anime, Fantasy, Science-Fiction]                       SUN-TV   \n",
       "11                   [Anime, Fantasy, Horror]                          NTV   \n",
       "12                                   [Nature]                      BBC One   \n",
       "13                           [Drama, Romance]                      BBC One   \n",
       "14                                         []  National Geographic Channel   \n",
       "15                        [Action, Adventure]                        Starz   \n",
       "16                            [Drama, Comedy]                           NA   \n",
       "17       [Action, Adventure, Science-Fiction]              Cartoon Network   \n",
       "18                   [Drama, Crime, Thriller]                   Sky Cinema   \n",
       "19                                    [Drama]                       Canal+   \n",
       "20                            [Drama, Sports]                          NBC   \n",
       "21                                    [Music]                          ABC   \n",
       "22                       [Adventure, Fantasy]                           NA   \n",
       "23                   [Drama, Crime, Thriller]                  USA Network   \n",
       "24                             [Drama, Crime]                           NA   \n",
       "25                     [Drama, Crime, Horror]                          NBC   \n",
       "26                           [Drama, History]                      BBC Two   \n",
       "27                                    [Drama]                         TVP1   \n",
       "28                                     [Food]                           NA   \n",
       "29                     [Drama, Action, Crime]                           NA   \n",
       "..                                        ...                          ...   \n",
       "196                                  [Comedy]                    Channel 4   \n",
       "197                   [Drama, Crime, Mystery]                      BBC One   \n",
       "198                    [Drama, Action, Crime]                           NA   \n",
       "199                          [Drama, History]                          HBO   \n",
       "200                            [War, History]                          ITV   \n",
       "201                 [Drama, Horror, Thriller]                           NA   \n",
       "202      [Action, Adventure, Science-Fiction]                           NA   \n",
       "203                                  [Comedy]                     BBC Four   \n",
       "204                                   [Drama]                           FX   \n",
       "205                   [Drama, Crime, History]                      BBC One   \n",
       "206         [Drama, Anime, Thriller, Mystery]                          NTV   \n",
       "207                   [Drama, Crime, Mystery]                     Showtime   \n",
       "208      [Action, Adventure, Science-Fiction]                          FOX   \n",
       "209                 [Drama, Mystery, Medical]                          FOX   \n",
       "210      [Action, Adventure, Science-Fiction]                       The WB   \n",
       "211                       [Comedy, Adventure]                   Adult Swim   \n",
       "212                  [Drama, Crime, Thriller]                           NA   \n",
       "213                         [Comedy, Romance]                    BBC Three   \n",
       "214        [Comedy, Action, Adventure, Anime]                          MBS   \n",
       "215                   [Drama, Comedy, Action]                           NA   \n",
       "216                   [Drama, Crime, Mystery]                          ITV   \n",
       "217                            [Drama, Crime]                          HBO   \n",
       "218          [Comedy, Music, Science-Fiction]                           NA   \n",
       "219                [Action, Adventure, Anime]                       Animax   \n",
       "220       [Action, Adventure, Anime, Fantasy]                          NTV   \n",
       "221                                  [Comedy]               Comedy Central   \n",
       "222                                  [Comedy]                        truTV   \n",
       "223                                   [Drama]                          HBO   \n",
       "224                            [War, History]                          PBS   \n",
       "225       [Drama, Adventure, Science-Fiction]                          FOX   \n",
       "\n",
       "    premiere_date            status  \n",
       "0      2006-10-05             Ended  \n",
       "1      2004-05-20             Ended  \n",
       "2      2017-03-17           Running  \n",
       "3      1997-08-13           Running  \n",
       "4      2016-11-18           Running  \n",
       "5      2017-10-29             Ended  \n",
       "6      2018-03-26           Running  \n",
       "7      1986-01-09             Ended  \n",
       "8      2005-02-21             Ended  \n",
       "9      2018-11-18           Running  \n",
       "10     2011-04-06             Ended  \n",
       "11     1997-10-07             Ended  \n",
       "12     2009-10-12             Ended  \n",
       "13     1995-09-24             Ended  \n",
       "14     1980-09-28             Ended  \n",
       "15     2011-01-21             Ended  \n",
       "16     2016-01-30             Ended  \n",
       "17     2004-07-31             Ended  \n",
       "18     2008-11-10             Ended  \n",
       "19     2015-04-27           Running  \n",
       "20     2006-10-03             Ended  \n",
       "21     1995-11-11             Ended  \n",
       "22     2015-03-13           Running  \n",
       "23     2015-06-24             Ended  \n",
       "24     2014-12-28           Running  \n",
       "25     2013-04-04             Ended  \n",
       "26     1976-09-20             Ended  \n",
       "27     1989-12-10             Ended  \n",
       "28     2015-04-26           Running  \n",
       "29     2018-07-06           Running  \n",
       "..            ...               ...  \n",
       "196    2000-09-29             Ended  \n",
       "197    2010-07-25  To Be Determined  \n",
       "198    2015-08-28             Ended  \n",
       "199    2019-05-06             Ended  \n",
       "200    1973-10-31             Ended  \n",
       "201    2018-10-12           Running  \n",
       "202    2010-11-26           Running  \n",
       "203    2014-10-02             Ended  \n",
       "204    2017-03-05             Ended  \n",
       "205    2013-09-12           Running  \n",
       "206    2006-10-03             Ended  \n",
       "207    2006-10-01             Ended  \n",
       "208    1992-09-05             Ended  \n",
       "209    2004-11-16             Ended  \n",
       "210    1997-09-13             Ended  \n",
       "211    2004-08-07           Running  \n",
       "212    2017-10-13             Ended  \n",
       "213    2000-05-12             Ended  \n",
       "214    2016-04-03           Running  \n",
       "215    2018-05-02           Running  \n",
       "216    1989-01-08             Ended  \n",
       "217    1999-01-10             Ended  \n",
       "218    2008-07-15             Ended  \n",
       "219    2003-10-04             Ended  \n",
       "220    2011-10-02             Ended  \n",
       "221    2013-02-28             Ended  \n",
       "222    2011-12-15           Running  \n",
       "223    1998-04-05             Ended  \n",
       "224    2017-09-17             Ended  \n",
       "225    2002-09-20             Ended  \n",
       "\n",
       "[226 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the above function #\n",
    "for entry in entries:\n",
    "    get_entry(entry)\n",
    "    \n",
    "shows_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 226 entries, 0 to 225\n",
      "Data columns (total 6 columns):\n",
      "show_name        226 non-null object\n",
      "rating_avg       203 non-null float64\n",
      "genres           226 non-null object\n",
      "network          226 non-null object\n",
      "premiere_date    226 non-null object\n",
      "status           226 non-null object\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 12.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Let's check for null values present, as well as data types for each column #\n",
    "shows_df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Let's continue with NLP using the Twitter API. Here, we'll create a method to pull a list of tweets from the Twitter API and attempt to classify whether a tweet comes from Sanders or Trump. We will begin with the Twitter API key setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "twitter_keys = {\n",
    "    'consumer_key' : 'AdhLIB5ImYxfv5SftZUO32xRd',\n",
    "    'consumer_secret' : 'WartHHi6xoaFD7FP1KCw3cURJqDRC3yjwUwWvcXpM5fMCdNRgJ',\n",
    "    'access_token_key' : '1258789060635701253-RMGwPtN5T2Oqn9Yv4Fekj6bzz9hDPX',\n",
    "    'access_token_secret' : 'qHryBE2Lf0qdQCJB8cM5NFGZjNj8pvzT4hgeRO5fYllyv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Let's create a class to make requests and eventually transform the JSON responses into DataFrames #\n",
    "api = twitter.Api(\n",
    "    consumer_key = twitter_keys['consumer_key'],\n",
    "    consumer_secret = twitter_keys['consumer_secret'],\n",
    "    access_token_key = twitter_keys['access_token_key'],\n",
    "    access_token_secret = twitter_keys['access_token_secret']\n",
    ")\n",
    "\n",
    "class TweetMiner(object):\n",
    "    result_limit = 20    \n",
    "    api = False\n",
    "    data = []\n",
    "    \n",
    "    def __init__(self, keys_dict, api, result_limit = 20):\n",
    "        self.api = api\n",
    "        self.twitter_keys = keys_dict\n",
    "        self.result_limit = result_limit\n",
    "        \n",
    "    def mine_user_tweets(self, user='DAndaluz', mine_rewteets=False, max_pages=5):\n",
    "        data = []\n",
    "        last_tweet_id = False\n",
    "        page = 1\n",
    "        \n",
    "        while page <= max_pages:\n",
    "\n",
    "            if last_tweet_id:\n",
    "                statuses = self.api.GetUserTimeline(screen_name=user, count=self.result_limit, max_id=last_tweet_id - 1)        \n",
    "            else:\n",
    "                statuses = self.api.GetUserTimeline(screen_name=user, count=self.result_limit)\n",
    "                \n",
    "            for item in statuses:\n",
    "                mined = {\n",
    "                    'tweet_id' : item.id,\n",
    "                    'handle' : item.user.name,\n",
    "                    'retweet_count' : item.retweet_count,\n",
    "                    'text' : item.text,\n",
    "                    'mined_at' : datetime.datetime.now(),\n",
    "                    'created_at' : item.created_at,\n",
    "                }\n",
    "            \n",
    "                last_tweet_id = item.id\n",
    "                data.append(mined)\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the class #\n",
    "miner = TweetMiner(twitter_keys, api, result_limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's mine the tweets from two Twitter users: Sanders and Trump #\n",
    "sanders = miner.mine_user_tweets(user=\"berniesanders\", max_pages=5)\n",
    "trump = miner.mine_user_tweets(user=\"realDonaldTrump\", max_pages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet_id': 1259963746899955715, 'handle': 'Bernie Sanders', 'retweet_count': 258, 'text': 'I find it interesting that now that the coronavirus has hit the White House, Donald Trump is now suddenly a big sup https://t.co/LcL8mJtTTx', 'mined_at': datetime.datetime(2020, 5, 11, 17, 53, 7, 367511), 'created_at': 'Mon May 11 21:49:00 +0000 2020'}\n"
     ]
    }
   ],
   "source": [
    "# Let's check out the first tweet in our list for Sanders #\n",
    "print(sanders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet_id': 1259940503598125057, 'handle': 'Donald J. Trump', 'retweet_count': 4269, 'text': 'RT @WhiteHouse: LIVE: President @realDonaldTrump delivers remarks on testing https://t.co/m2HCbBcA5o', 'mined_at': datetime.datetime(2020, 5, 11, 17, 53, 7, 922836), 'created_at': 'Mon May 11 20:16:39 +0000 2020'}\n"
     ]
    }
   ],
   "source": [
    "# Let's check out the first tweet in our list for Trump #\n",
    "print(trump[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>handle</th>\n",
       "      <th>mined_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon May 11 21:49:00 +0000 2020</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2020-05-11 17:53:07.367511</td>\n",
       "      <td>258</td>\n",
       "      <td>I find it interesting that now that the corona...</td>\n",
       "      <td>1259963746899955715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon May 11 18:42:27 +0000 2020</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2020-05-11 17:53:07.367511</td>\n",
       "      <td>1420</td>\n",
       "      <td>In April, 26.4 percent of workers lost their j...</td>\n",
       "      <td>1259916797425463296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon May 11 15:48:34 +0000 2020</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2020-05-11 17:53:07.467841</td>\n",
       "      <td>2167</td>\n",
       "      <td>While working people struggle to survive, the ...</td>\n",
       "      <td>1259873039682076674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon May 11 15:11:51 +0000 2020</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2020-05-11 17:53:07.467841</td>\n",
       "      <td>436</td>\n",
       "      <td>RT @postlive: Sen. Bernie Sanders says \"health...</td>\n",
       "      <td>1259863800662183941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon May 11 15:08:59 +0000 2020</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2020-05-11 17:53:07.583834</td>\n",
       "      <td>189</td>\n",
       "      <td>Join me live now with the @washingtonpost to d...</td>\n",
       "      <td>1259863079493517312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at          handle                   mined_at  \\\n",
       "0  Mon May 11 21:49:00 +0000 2020  Bernie Sanders 2020-05-11 17:53:07.367511   \n",
       "1  Mon May 11 18:42:27 +0000 2020  Bernie Sanders 2020-05-11 17:53:07.367511   \n",
       "2  Mon May 11 15:48:34 +0000 2020  Bernie Sanders 2020-05-11 17:53:07.467841   \n",
       "3  Mon May 11 15:11:51 +0000 2020  Bernie Sanders 2020-05-11 17:53:07.467841   \n",
       "4  Mon May 11 15:08:59 +0000 2020  Bernie Sanders 2020-05-11 17:53:07.583834   \n",
       "\n",
       "   retweet_count                                               text  \\\n",
       "0            258  I find it interesting that now that the corona...   \n",
       "1           1420  In April, 26.4 percent of workers lost their j...   \n",
       "2           2167  While working people struggle to survive, the ...   \n",
       "3            436  RT @postlive: Sen. Bernie Sanders says \"health...   \n",
       "4            189  Join me live now with the @washingtonpost to d...   \n",
       "\n",
       "              tweet_id  \n",
       "0  1259963746899955715  \n",
       "1  1259916797425463296  \n",
       "2  1259873039682076674  \n",
       "3  1259863800662183941  \n",
       "4  1259863079493517312  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's convert the tweet ouputs to a DataFrame #\n",
    "pd.DataFrame(sanders).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the class and mine the tweets from Sanders and Trump #\n",
    "miner = TweetMiner(twitter_keys, api, result_limit=400)\n",
    "sanders_tweets = miner.mine_user_tweets('berniesanders')\n",
    "trump_tweets = miner.mine_user_tweets('realDonaldTrump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1000 \n",
      "Columns: 6\n"
     ]
    }
   ],
   "source": [
    "# Let's create a DataFrame for Sanders #\n",
    "sanders_df = pd.DataFrame(sanders_tweets) \n",
    "print('Rows: %s \\nColumns: %s' %(sanders_df.shape[0], sanders_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 599 \n",
      "Columns: 6\n"
     ]
    }
   ],
   "source": [
    "# Let's create a DataFrame for Trump #\n",
    "trump_df = pd.DataFrame(trump_tweets) \n",
    "print('Rows: %s \\nColumns: %s' %(trump_df.shape[0], trump_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1599 \n",
      "Columns: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>handle</th>\n",
       "      <th>mined_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>Thu Apr 30 12:32:50 +0000 2020</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2020-05-11 17:53:10.848994</td>\n",
       "      <td>17805</td>\n",
       "      <td>RT @realDonaldTrump: Despite reports to the co...</td>\n",
       "      <td>1255837513559822338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Sat May 02 21:25:45 +0000 2020</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2020-05-11 17:53:10.848994</td>\n",
       "      <td>5330</td>\n",
       "      <td>RT @Alyssafarah: The Trump Admin is making sur...</td>\n",
       "      <td>1256696404917059585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Tue Feb 18 01:36:39 +0000 2020</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2020-05-11 17:53:10.077327</td>\n",
       "      <td>7201</td>\n",
       "      <td>Together, we are going to end the greed of the...</td>\n",
       "      <td>1229580453574823937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Thu Apr 30 11:45:41 +0000 2020</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2020-05-11 17:53:11.238465</td>\n",
       "      <td>17805</td>\n",
       "      <td>Despite reports to the contrary, Sweden is pay...</td>\n",
       "      <td>1255825648448348161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Fri May 08 20:15:43 +0000 2020</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2020-05-11 17:53:10.510074</td>\n",
       "      <td>14981</td>\n",
       "      <td>https://t.co/ykwoWfQfCf</td>\n",
       "      <td>1258853107573891075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Fri May 01 04:36:58 +0000 2020</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2020-05-11 17:53:10.848994</td>\n",
       "      <td>6884</td>\n",
       "      <td>RT @hughhewitt: The always careful @EliLake ...</td>\n",
       "      <td>1256080145053614081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>Sat Feb 15 23:31:49 +0000 2020</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2020-05-11 17:53:10.077327</td>\n",
       "      <td>676</td>\n",
       "      <td>RT @kailanikm: Bernie Sanders is rallying a cr...</td>\n",
       "      <td>1228824262993219584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Wed Apr 08 15:20:18 +0000 2020</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2020-05-11 17:53:08.757910</td>\n",
       "      <td>4278</td>\n",
       "      <td>Please join me at 11:45 a.m. ET for a special ...</td>\n",
       "      <td>1247907128024666114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Sat May 09 15:53:36 +0000 2020</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>2020-05-11 17:53:10.510074</td>\n",
       "      <td>6667</td>\n",
       "      <td>RT @MikeGarcia2020: 2/The right to vote is sac...</td>\n",
       "      <td>1259149530240598016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Wed Mar 25 21:59:56 +0000 2020</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2020-05-11 17:53:09.128129</td>\n",
       "      <td>1190</td>\n",
       "      <td>Please join me at 7 p.m. ET tonight to discuss...</td>\n",
       "      <td>1242934267543420930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         created_at           handle  \\\n",
       "393  Thu Apr 30 12:32:50 +0000 2020  Donald J. Trump   \n",
       "301  Sat May 02 21:25:45 +0000 2020  Donald J. Trump   \n",
       "951  Tue Feb 18 01:36:39 +0000 2020   Bernie Sanders   \n",
       "418  Thu Apr 30 11:45:41 +0000 2020  Donald J. Trump   \n",
       "185  Fri May 08 20:15:43 +0000 2020  Donald J. Trump   \n",
       "356  Fri May 01 04:36:58 +0000 2020  Donald J. Trump   \n",
       "983  Sat Feb 15 23:31:49 +0000 2020   Bernie Sanders   \n",
       "147  Wed Apr 08 15:20:18 +0000 2020   Bernie Sanders   \n",
       "175  Sat May 09 15:53:36 +0000 2020  Donald J. Trump   \n",
       "270  Wed Mar 25 21:59:56 +0000 2020   Bernie Sanders   \n",
       "\n",
       "                      mined_at  retweet_count  \\\n",
       "393 2020-05-11 17:53:10.848994          17805   \n",
       "301 2020-05-11 17:53:10.848994           5330   \n",
       "951 2020-05-11 17:53:10.077327           7201   \n",
       "418 2020-05-11 17:53:11.238465          17805   \n",
       "185 2020-05-11 17:53:10.510074          14981   \n",
       "356 2020-05-11 17:53:10.848994           6884   \n",
       "983 2020-05-11 17:53:10.077327            676   \n",
       "147 2020-05-11 17:53:08.757910           4278   \n",
       "175 2020-05-11 17:53:10.510074           6667   \n",
       "270 2020-05-11 17:53:09.128129           1190   \n",
       "\n",
       "                                                  text             tweet_id  \n",
       "393  RT @realDonaldTrump: Despite reports to the co...  1255837513559822338  \n",
       "301  RT @Alyssafarah: The Trump Admin is making sur...  1256696404917059585  \n",
       "951  Together, we are going to end the greed of the...  1229580453574823937  \n",
       "418  Despite reports to the contrary, Sweden is pay...  1255825648448348161  \n",
       "185                            https://t.co/ykwoWfQfCf  1258853107573891075  \n",
       "356  RT @hughhewitt: The always careful @EliLake ...  1256080145053614081  \n",
       "983  RT @kailanikm: Bernie Sanders is rallying a cr...  1228824262993219584  \n",
       "147  Please join me at 11:45 a.m. ET for a special ...  1247907128024666114  \n",
       "175  RT @MikeGarcia2020: 2/The right to vote is sac...  1259149530240598016  \n",
       "270  Please join me at 7 p.m. ET tonight to discuss...  1242934267543420930  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's concat the two DataFrames #\n",
    "tweets = pd.concat([sanders_df, trump_df], axis=0)\n",
    "print('Rows: %s \\nColumns: %s' %(tweets.shape[0], tweets.shape[1]))\n",
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a vectorizer and figure out what the most common ngrams are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https co', 794),\n",
       " ('health care', 64),\n",
       " ('on the', 63),\n",
       " ('of the', 58),\n",
       " ('in the', 57),\n",
       " ('to the', 46),\n",
       " ('we are', 46),\n",
       " ('for all', 39),\n",
       " ('we need', 39),\n",
       " ('for the', 38),\n",
       " ('we have', 38),\n",
       " ('if you', 37),\n",
       " ('we re', 35),\n",
       " ('going to', 35),\n",
       " ('we will', 35),\n",
       " ('the coronavirus', 31),\n",
       " ('we must', 30),\n",
       " ('need to', 27),\n",
       " ('we can', 26),\n",
       " ('and the', 26)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use the TfidfVectorizer to find ngrams for us #\n",
    "vect = TfidfVectorizer(ngram_range=(2,4))\n",
    "\n",
    "# Let's pull all of Sanders' tweet texts into one giant string #\n",
    "summaries = ''.join(sanders_df['text'])\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngrams_summaries).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https co', 308),\n",
       " ('of the', 41),\n",
       " ('in the', 31),\n",
       " ('president realdonaldtrump', 27),\n",
       " ('fake news', 25),\n",
       " ('for the', 24),\n",
       " ('thank you', 21),\n",
       " ('rt johnwhuber', 20),\n",
       " ('rt realdonaldtrump', 15),\n",
       " ('will be', 15),\n",
       " ('the people', 14),\n",
       " ('people of', 12),\n",
       " ('to be', 12),\n",
       " ('should be', 11),\n",
       " ('has been', 11),\n",
       " ('on the', 11),\n",
       " ('to the', 10),\n",
       " ('do nothing', 10),\n",
       " ('rt cdcgov', 10),\n",
       " ('rt whitehouse', 9)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's pull all of Trump's tweet texts into one giant string #\n",
    "summaries = ''.join(trump_df['text'])\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngrams_summaries).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process the tweets and build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the textacy package to do some more comprehensive preprocessing #\n",
    "tweet_text = tweets['text'].values\n",
    "\n",
    "clean_text = [x.lower() for x in tweet_text]\n",
    "\n",
    "clean_text = [preprocessing.replace.replace_hashtags(x, replace_with='') for x in clean_text]\n",
    "\n",
    "clean_text = [preprocessing.replace.replace_urls(x, replace_with='') for x in clean_text]\n",
    "\n",
    "clean_text = [preprocessing.normalize.normalize_unicode(x) for x in clean_text]\n",
    "\n",
    "clean_text = [preprocessing.normalize.normalize_whitespace(x) for x in clean_text]\n",
    "\n",
    "clean_text = [preprocessing.replace.replace_currency_symbols(x, replace_with='') for x in clean_text]\n",
    "\n",
    "clean_text = [preprocessing.replace.replace_phone_numbers(x, replace_with='') for x in clean_text]\n",
    "\n",
    "clean_text = [preprocessing.replace.replace_emails(x, replace_with='') for x in clean_text]\n",
    "\n",
    "clean_text = [preprocessing.replace.replace_emojis(x, replace_with='') for x in clean_text]\n",
    "\n",
    "clean_text = [preprocessing.remove.remove_accents(x) for x in clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I find it interesting that now that the coronavirus has hit the White House, Donald Trump is now suddenly a big sup https://t.co/LcL8mJtTTx'\n",
      " 'In April, 26.4 percent of workers lost their jobs or had their hours reduced. \\n\\nDuring this horrific crisis, we hav https://t.co/196FDdUzUz'\n",
      " 'While working people struggle to survive, the rich reach unthinkable levels of wealth.\\n\\nThe annual cost of chemothe https://t.co/eQwrFVpC5T']\n"
     ]
    }
   ],
   "source": [
    "# Let's check out the first three tweets before preprocessing #\n",
    "print(tweet_text[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i find it interesting that now that the coronavirus has hit the white house, donald trump is now suddenly a big sup...', 'in april, 26.4 percent of workers lost their jobs or had their hours reduced. \\nduring this horrific crisis, we hav...', 'while working people struggle to survive, the rich reach unthinkable levels of wealth.\\nthe annual cost of chemothe...']\n"
     ]
    }
   ],
   "source": [
    "# Let's check out the first three tweets after preprocessing #\n",
    "print(clean_text[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target mean (baseline): 0.37460913070669166\n"
     ]
    }
   ],
   "source": [
    "# Let's make the user handle our target (Sanders will be 0 and Trump will be 1) #\n",
    "\n",
    "y = tweets['handle'].map(lambda x: 0 if x == 'Bernie Sanders' else 1).values\n",
    "\n",
    "print('Target mean (baseline):', np.mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1599 \n",
      "Columns: 2000\n"
     ]
    }
   ],
   "source": [
    "# Let's preprocess our text data to tfidf #\n",
    "tfv = TfidfVectorizer(ngram_range=(1,4), max_features=2000)\n",
    "X = tfv.fit_transform(clean_text).todense()\n",
    "\n",
    "print('Rows: %s \\nColumns: %s' %(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of accuracies: [0.925      0.9        0.85625    0.8125     0.8375     0.7875\n",
      " 0.86875    0.84375    0.83125    0.86163522]\n",
      "\n",
      "Mean of accuracies: 0.8524135220125787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's cross-validate the accuracy #\n",
    "accuracies = cross_val_score(LogisticRegression(solver='lbfgs'), X, y, cv=10)\n",
    "\n",
    "print('List of accuracies:', accuracies)\n",
    "print('\\nMean of accuracies:', np.mean(accuracies))\n",
    "\n",
    "# Let's instantiate and fit our logistic regression model #\n",
    "estimator = LogisticRegression(solver='lbfgs')\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very good accuracy considering the baseline.\n",
    "\n",
    "Let's check the predicted probability for a random tweet from both Sanders and Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76465727, 0.23534273],\n",
       "       [0.23585026, 0.76414974]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's set our source as tfidf vectors #\n",
    "source_test = [sanders_df['text'][np.random.choice(len(sanders_df['text']))],\n",
    "               trump_df['text'][np.random.choice(len(trump_df['text']))]]\n",
    "\n",
    "Xtest = tfv.transform(source_test)\n",
    "\n",
    "# Let's predict using the previously trained model #\n",
    "estimator.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that our classifier is predicting correctly. The 1st column is probability of being Bernie, and 2nd Trump."
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
